#!/usr/bin/env python3
"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞

–°–æ–±–∏—Ä–∞–µ—Ç –¢–û–õ–¨–ö–û –Ω—É–∂–Ω—É—é –∏ –í–°–Æ –Ω—É–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è 
–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–ª–∞–Ω–∞ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ (10/10).

–ü—Ä–∏–Ω—Ü–∏–ø—ã:
1. –§–æ–∫—É—Å –Ω–∞ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–µ - –Ω–µ —Å–æ–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–µ–µ
2. –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö - —Ä–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –Ω–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ
3. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å - —á–µ—Ç–∫–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
4. –≠–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç—å - –≥–æ—Ç–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞
"""

import sys
import json
import ast
from pathlib import Path
from typing import Dict, List, Optional
from collections import defaultdict, Counter

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from contextual_file_analyzer import ContextualFileAnalyzer


class OptimizedRefactoringAnalyzer(ContextualFileAnalyzer):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–ª–∞–Ω–∞ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞"""

    def __init__(self, project_path: str, target_file: str, output_dir: str, verbose: bool = False):
        super().__init__(project_path, target_file, output_dir, verbose)
        
        self.analysis_mode = "optimized_refactoring_analysis"
        self.logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω OptimizedRefactoringAnalyzer")
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self.refactoring_data = {
            'file_structure': {},
            'real_usage_patterns': {},
            'api_contracts': {},
            'data_schemas': {},
            'refactoring_opportunities': {},
            'expert_recommendations': {}
        }

    def run_optimized_analysis(self):
        """–ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞"""
        self.logger.info("[–°–¢–ê–†–¢] –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞...")

        # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–æ–≤ - —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ–µ –¥–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞
        analyses = [
            ("–°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞", self.analyze_file_structure),
            ("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è", self.extract_real_usage_patterns),
            ("–ê–Ω–∞–ª–∏–∑ API –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤", self.analyze_api_contracts),
            ("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ö–µ–º –¥–∞–Ω–Ω—ã—Ö", self.extract_data_schemas),
            ("–í—ã—è–≤–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞", self.identify_refactoring_opportunities),
            ("–°–æ–∑–¥–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π", self.create_expert_recommendations),
            ("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –ø–ª–∞–Ω–∞", self.generate_refactoring_plan)
        ]

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑—ã
        for analysis_name, analysis_func in analyses:
            try:
                self.logger.info(f"[–í–´–ü–û–õ–ù–ï–ù–ò–ï] {analysis_name}")
                success = analysis_func()
                if success:
                    self.logger.info(f"[–£–°–ü–ï–•] {analysis_name}")
                else:
                    self.logger.warning(f"[–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï] {analysis_name}")
            except Exception as e:
                self.logger.error(f"[–û–®–ò–ë–ö–ê] {analysis_name}: {e}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.save_optimized_results()
        return True

    def analyze_file_structure(self):
        """1. –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞ - –æ—Å–Ω–æ–≤–∞ –¥–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞"""
        self.logger.info("–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ñ–∞–π–ª–∞...")
        
        try:
            with open(self.target_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –ü–∞—Ä—Å–∏–º AST
            tree = ast.parse(content)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            structure = {
                'classes': [],
                'functions': [],
                'imports': [],
                'constants': [],
                'complexity_metrics': {}
            }
            
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    class_info = {
                        'name': node.name,
                        'methods': [n.name for n in node.body if isinstance(n, ast.FunctionDef)],
                        'line_start': node.lineno,
                        'line_end': getattr(node, 'end_lineno', node.lineno),
                        'method_count': len([n for n in node.body if isinstance(n, ast.FunctionDef)]),
                        'is_god_object': len([n for n in node.body if isinstance(n, ast.FunctionDef)]) > 15
                    }
                    structure['classes'].append(class_info)
                
                elif isinstance(node, ast.FunctionDef) and not any(
                    node.lineno >= cls['line_start'] and node.lineno <= cls['line_end'] 
                    for cls in structure['classes']
                ):
                    func_info = {
                        'name': node.name,
                        'line_start': node.lineno,
                        'line_end': getattr(node, 'end_lineno', node.lineno),
                        'args': [arg.arg for arg in node.args.args],
                        'is_large': (getattr(node, 'end_lineno', node.lineno) - node.lineno) > 50
                    }
                    structure['functions'].append(func_info)
                
                elif isinstance(node, (ast.Import, ast.ImportFrom)):
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            structure['imports'].append({
                                'type': 'import',
                                'module': alias.name,
                                'alias': alias.asname
                            })
                    else:
                        for alias in node.names:
                            structure['imports'].append({
                                'type': 'from_import',
                                'module': node.module,
                                'name': alias.name,
                                'alias': alias.asname
                            })
            
            # –ú–µ—Ç—Ä–∏–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            structure['complexity_metrics'] = {
                'total_classes': len(structure['classes']),
                'total_functions': len(structure['functions']),
                'total_imports': len(structure['imports']),
                'god_objects': [cls['name'] for cls in structure['classes'] if cls['is_god_object']],
                'large_functions': [func['name'] for func in structure['functions'] if func['is_large']],
                'lines_of_code': len(content.splitlines())
            }
            
            self.refactoring_data['file_structure'] = structure
            return True
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {e}")
            return False

    def extract_real_usage_patterns(self):
        """2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–∑ –∫–æ–¥–∞"""
        self.logger.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è...")
        
        # –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å–∫–∞–µ–º –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        self.build_project_index_safe()
        
        # –ò—â–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –≤—ã–∑–æ–≤—ã –≤ –ø—Ä–æ–µ–∫—Ç–µ
        usage_patterns = {
            'method_calls': [],
            'parameter_patterns': {},
            'return_value_usage': [],
            'error_handling_patterns': []
        }
        
        try:
            # –ò—â–µ–º —Ñ–∞–π–ª—ã Python –≤ –ø—Ä–æ–µ–∫—Ç–µ
            python_files = list(self.project_path.rglob("*.py"))
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—ã–∑–æ–≤—ã —Ü–µ–ª–µ–≤–æ–≥–æ —Ñ–∞–π–ª–∞
            target_module_name = self.target_file.stem
            
            for py_file in python_files[:50]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # –ò—â–µ–º –∏–º–ø–æ—Ä—Ç—ã –Ω–∞—à–µ–≥–æ –º–æ–¥—É–ª—è
                    if target_module_name in content:
                        # –ü–∞—Ä—Å–∏–º –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤
                        try:
                            tree = ast.parse(content)
                            for node in ast.walk(tree):
                                if isinstance(node, ast.Call):
                                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–∑–æ–≤–µ
                                    call_info = self._extract_call_info(node, py_file)
                                    if call_info:
                                        usage_patterns['method_calls'].append(call_info)
                        except:
                            continue
                            
                except Exception:
                    continue
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            usage_patterns['parameter_patterns'] = self._analyze_parameter_patterns(
                usage_patterns['method_calls']
            )
            
            self.refactoring_data['real_usage_patterns'] = usage_patterns
            return True
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {e}")
            return False

    def _extract_call_info(self, node: ast.Call, file_path: Path) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–∑–æ–≤–µ —Ñ—É–Ω–∫—Ü–∏–∏"""
        try:
            call_info = {
                'file': str(file_path.relative_to(self.project_path)),
                'line': node.lineno,
                'function_name': None,
                'args': [],
                'kwargs': {}
            }
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è —Ñ—É–Ω–∫—Ü–∏–∏
            if isinstance(node.func, ast.Name):
                call_info['function_name'] = node.func.id
            elif isinstance(node.func, ast.Attribute):
                call_info['function_name'] = node.func.attr
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã
            for arg in node.args:
                if isinstance(arg, ast.Constant):
                    call_info['args'].append(arg.value)
                elif isinstance(arg, ast.Name):
                    call_info['args'].append(f"<var:{arg.id}>")
                else:
                    call_info['args'].append(f"<expr:{type(arg).__name__}>")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º keyword –∞—Ä–≥—É–º–µ–Ω—Ç—ã
            for keyword in node.keywords:
                if isinstance(keyword.value, ast.Constant):
                    call_info['kwargs'][keyword.arg] = keyword.value.value
                elif isinstance(keyword.value, ast.Name):
                    call_info['kwargs'][keyword.arg] = f"<var:{keyword.value.id}>"
                else:
                    call_info['kwargs'][keyword.arg] = f"<expr:{type(keyword.value).__name__}>"
            
            return call_info
            
        except Exception:
            return None

    def _analyze_parameter_patterns(self, method_calls: List[Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        patterns = {
            'common_parameters': Counter(),
            'parameter_types': defaultdict(Counter),
            'real_values': defaultdict(set)
        }
        
        for call in method_calls:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º kwargs
            for param, value in call.get('kwargs', {}).items():
                patterns['common_parameters'][param] += 1
                
                if isinstance(value, str) and not value.startswith('<'):
                    patterns['real_values'][param].add(value)
                    patterns['parameter_types'][param]['string'] += 1
                elif isinstance(value, (int, float)):
                    patterns['real_values'][param].add(value)
                    patterns['parameter_types'][param]['number'] += 1
                elif isinstance(value, bool):
                    patterns['real_values'][param].add(value)
                    patterns['parameter_types'][param]['boolean'] += 1
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º sets –≤ lists –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        for param in patterns['real_values']:
            patterns['real_values'][param] = list(patterns['real_values'][param])
        
        return patterns

    def analyze_api_contracts(self):
        """3. –ê–Ω–∞–ª–∏–∑ API –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ –∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤"""
        self.logger.info("–ê–Ω–∞–ª–∏–∑ API –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤...")
        
        contracts = {
            'public_methods': [],
            'method_signatures': {},
            'return_types': {},
            'exception_contracts': {},
            'dependencies': []
        }
        
        try:
            with open(self.target_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç–æ–¥—ã –∏ –∏—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    method_info = {
                        'name': node.name,
                        'is_public': not node.name.startswith('_'),
                        'args': [arg.arg for arg in node.args.args],
                        'defaults': len(node.args.defaults),
                        'has_docstring': ast.get_docstring(node) is not None,
                        'raises_exceptions': []
                    }
                    
                    # –ò—â–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
                    for child in ast.walk(node):
                        if isinstance(child, ast.Raise):
                            if isinstance(child.exc, ast.Call) and isinstance(child.exc.func, ast.Name):
                                method_info['raises_exceptions'].append(child.exc.func.id)
                    
                    if method_info['is_public']:
                        contracts['public_methods'].append(method_info)
                    
                    contracts['method_signatures'][node.name] = method_info
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        contracts['dependencies'].append({
                            'type': 'import',
                            'module': alias.name
                        })
                elif isinstance(node, ast.ImportFrom):
                    for alias in node.names:
                        contracts['dependencies'].append({
                            'type': 'from_import',
                            'module': node.module,
                            'name': alias.name
                        })
            
            self.refactoring_data['api_contracts'] = contracts
            return True
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤: {e}")
            return False

    def extract_data_schemas(self):
        """4. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ö–µ–º –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–∏–ø–æ–≤"""
        self.logger.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ö–µ–º –¥–∞–Ω–Ω—ã—Ö...")
        
        schemas = {
            'type_definitions': {},
            'data_structures': {},
            'constants': {},
            'enums': []
        }
        
        try:
            with open(self.target_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            
            # –ò—â–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–æ–≤ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä –¥–∞–Ω–Ω—ã—Ö
            for node in ast.walk(tree):
                if isinstance(node, ast.Assign):
                    for target in node.targets:
                        if isinstance(target, ast.Name):
                            # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã (UPPER_CASE)
                            if target.id.isupper():
                                if isinstance(node.value, ast.Constant):
                                    schemas['constants'][target.id] = node.value.value
                            
                            # TypeAlias –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
                            if 'TypeAlias' in ast.dump(node) or target.id.endswith('Type'):
                                schemas['type_definitions'][target.id] = ast.dump(node.value)
                
                elif isinstance(node, ast.ClassDef):
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Å—ã –∫–∞–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
                    class_schema = {
                        'name': node.name,
                        'attributes': [],
                        'methods': [],
                        'is_dataclass': any(
                            isinstance(decorator, ast.Name) and decorator.id == 'dataclass'
                            for decorator in node.decorator_list
                        )
                    }
                    
                    for item in node.body:
                        if isinstance(item, ast.AnnAssign) and isinstance(item.target, ast.Name):
                            class_schema['attributes'].append({
                                'name': item.target.id,
                                'type': ast.dump(item.annotation) if item.annotation else None
                            })
                        elif isinstance(item, ast.FunctionDef):
                            class_schema['methods'].append(item.name)
                    
                    schemas['data_structures'][node.name] = class_schema
            
            self.refactoring_data['data_schemas'] = schemas
            return True
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ö–µ–º: {e}")
            return False

    def identify_refactoring_opportunities(self):
        """5. –í—ã—è–≤–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞"""
        self.logger.info("–í—ã—è–≤–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞...")
        
        opportunities = {
            'god_objects': [],
            'large_methods': [],
            'duplicate_code': [],
            'complex_conditionals': [],
            'long_parameter_lists': [],
            'feature_envy': [],
            'dead_code': []
        }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        structure = self.refactoring_data.get('file_structure', {})
        
        # God Objects
        for cls in structure.get('classes', []):
            if cls.get('is_god_object', False):
                opportunities['god_objects'].append({
                    'class': cls['name'],
                    'method_count': cls['method_count'],
                    'recommendation': '–†–∞–∑–¥–µ–ª–∏—Ç—å –∫–ª–∞—Å—Å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤',
                    'priority': 'HIGH'
                })
        
        # Large Methods
        for func in structure.get('functions', []):
            if func.get('is_large', False):
                opportunities['large_methods'].append({
                    'function': func['name'],
                    'lines': func['line_end'] - func['line_start'],
                    'recommendation': '–†–∞–∑–±–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏',
                    'priority': 'MEDIUM'
                })
        
        # Long Parameter Lists
        contracts = self.refactoring_data.get('api_contracts', {})
        for method_name, method_info in contracts.get('method_signatures', {}).items():
            if len(method_info.get('args', [])) > 5:
                opportunities['long_parameter_lists'].append({
                    'method': method_name,
                    'parameter_count': len(method_info['args']),
                    'recommendation': '–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–ª–∏ —Ä–∞–∑–±–∏—Ç—å –º–µ—Ç–æ–¥',
                    'priority': 'MEDIUM'
                })
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã —á–µ—Ä–µ–∑ IntelliRefactor
        self._detect_duplicates_for_opportunities(opportunities)
        
        self.refactoring_data['refactoring_opportunities'] = opportunities
        return True

    def _detect_duplicates_for_opportunities(self, opportunities: Dict):
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞"""
        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            result = self._run_intellirefactor_command_with_timeout(
                ["duplicates", str(self.target_file), "--format", "json"],
                f"duplicates_for_refactoring_{self.timestamp}.json",
                timeout_minutes=5
            )
            
            if result.get("success") and result.get("output_file"):
                with open(result["output_file"], 'r', encoding='utf-8') as f:
                    duplicates_data = json.load(f)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö
                duplicates = duplicates_data.get('duplicates', {}).get('duplicates', [])
                
                for dup in duplicates[:10]:  # –ü–µ—Ä–≤—ã–µ 10 –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                    if isinstance(dup, dict):
                        opportunities['duplicate_code'].append({
                            'locations': dup.get('locations', []),
                            'similarity': dup.get('similarity_score', 0),
                            'lines': dup.get('lines', 0),
                            'recommendation': '–ò–∑–≤–ª–µ—á—å –æ–±—â–∏–π –∫–æ–¥ –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é',
                            'priority': 'HIGH' if dup.get('similarity_score', 0) > 0.8 else 'MEDIUM'
                        })
        
        except Exception as e:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã: {e}")

    def create_expert_recommendations(self):
        """6. –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        self.logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...")
        
        recommendations = {
            'priority_actions': [],
            'refactoring_strategy': {},
            'risk_assessment': {},
            'implementation_plan': {},
            'quality_metrics': {}
        }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ —Å–æ–∑–¥–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –ø–ª–∞–Ω
        opportunities = self.refactoring_data.get('refactoring_opportunities', {})
        
        # –í—ã—Å–æ–∫–æ–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
        high_priority = []
        
        if opportunities.get('god_objects'):
            high_priority.append({
                'action': '–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ God Objects',
                'reason': f"–ù–∞–π–¥–µ–Ω–æ {len(opportunities['god_objects'])} –∫–ª–∞—Å—Å–æ–≤ —Å –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é",
                'impact': '–í—ã—Å–æ–∫–∏–π - —É–ª—É—á—à–∏—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ—Å—Ç—å –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å',
                'effort': '–í—ã—Å–æ–∫–∏–π - —Ç—Ä–µ–±—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π'
            })
        
        if opportunities.get('duplicate_code'):
            high_priority.append({
                'action': '–£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∫–æ–¥–∞',
                'reason': f"–ù–∞–π–¥–µ–Ω–æ {len(opportunities['duplicate_code'])} –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤",
                'impact': '–°—Ä–µ–¥–Ω–∏–π - —É–º–µ–Ω—å—à–∏—Ç –æ–±—ä–µ–º –∫–æ–¥–∞ –∏ —É–ø—Ä–æ—Å—Ç–∏—Ç —Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏–µ',
                'effort': '–°—Ä–µ–¥–Ω–∏–π - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤ –æ–±—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏'
            })
        
        recommendations['priority_actions'] = high_priority
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞
        recommendations['refactoring_strategy'] = {
            'approach': '–ü–æ—ç—Ç–∞–ø–Ω—ã–π —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏',
            'phases': [
                '–§–∞–∑–∞ 1: –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –º–µ–ª–∫–∏—Ö –∑–∞–ø–∞—Ö–æ–≤ –∫–æ–¥–∞',
                '–§–∞–∑–∞ 2: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫—Ä—É–ø–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –∏ –º–µ—Ç–æ–¥–æ–≤',
                '–§–∞–∑–∞ 3: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤'
            ],
            'testing_strategy': '–°–æ–∑–¥–∞–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ –ø–µ—Ä–µ–¥ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–æ–º'
        }
        
        # –û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤
        structure = self.refactoring_data.get('file_structure', {})
        recommendations['risk_assessment'] = {
            'complexity_risk': 'HIGH' if structure.get('complexity_metrics', {}).get('lines_of_code', 0) > 1000 else 'MEDIUM',
            'dependency_risk': 'MEDIUM',  # –ú–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –∞–Ω–∞–ª–∏–∑–æ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            'testing_risk': 'HIGH',  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ç–µ—Å—Ç–æ–≤
            'mitigation_strategies': [
                '–°–æ–∑–¥–∞—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ —Ç–µ—Å—Ç—ã –ø–µ—Ä–µ–¥ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–æ–º',
                '–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥',
                '–ü—Ä–æ–≤–æ–¥–∏—Ç—å code review –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ'
            ]
        }
        
        self.refactoring_data['expert_recommendations'] = recommendations
        return True

    def generate_refactoring_plan(self):
        """7. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –ø–ª–∞–Ω–∞ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞"""
        self.logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –ø–ª–∞–Ω–∞ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞...")
        
        # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        plan_content = self._create_detailed_refactoring_plan()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–ª–∞–Ω
        plan_path = self.output_dir / f"OPTIMIZED_REFACTORING_PLAN_{self.timestamp}.md"
        with open(plan_path, 'w', encoding='utf-8') as f:
            f.write(plan_content)
        
        self.analysis_results["generated_files"].append(str(plan_path))
        self.logger.info(f"–ü–ª–∞–Ω —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ —Å–æ–∑–¥–∞–Ω: {plan_path}")
        
        return True

    def _create_detailed_refactoring_plan(self) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞"""
        structure = self.refactoring_data.get('file_structure', {})
        opportunities = self.refactoring_data.get('refactoring_opportunities', {})
        recommendations = self.refactoring_data.get('expert_recommendations', {})
        usage_patterns = self.refactoring_data.get('real_usage_patterns', {})
        
        try:
            relative_file_path = self.target_file.relative_to(self.project_path)
        except ValueError:
            relative_file_path = self.target_file
        
        return f"""# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞

**–§–∞–π–ª:** {relative_file_path}
**–ü—Ä–æ–µ–∫—Ç:** {self.project_path.name}
**–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞:** {self.timestamp}
**–¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞:** –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞

## üéØ –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ

–ü—Ä–æ–≤–µ–¥–µ–Ω —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–ª–∞–Ω–∞ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞.

### –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
- **–°—Ç—Ä–æ–∫ –∫–æ–¥–∞:** {structure.get('complexity_metrics', {}).get('lines_of_code', 'N/A')}
- **–ö–ª–∞—Å—Å–æ–≤:** {structure.get('complexity_metrics', {}).get('total_classes', 0)}
- **–§—É–Ω–∫—Ü–∏–π:** {structure.get('complexity_metrics', {}).get('total_functions', 0)}
- **God Objects:** {len(opportunities.get('god_objects', []))}
- **–ö—Ä—É–ø–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤:** {len(opportunities.get('large_methods', []))}
- **–î—É–±–ª–∏–∫–∞—Ç–æ–≤ –∫–æ–¥–∞:** {len(opportunities.get('duplicate_code', []))}

## üö® –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

### –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ (—Ç—Ä–µ–±—É—é—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è)
"""

        # –î–æ–±–∞–≤–ª—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
        critical_issues = []
        
        if opportunities.get('god_objects'):
            for god_obj in opportunities['god_objects']:
                critical_issues.append(f"**God Object: {god_obj['class']}** - {god_obj['method_count']} –º–µ—Ç–æ–¥–æ–≤")
        
        if opportunities.get('duplicate_code'):
            high_similarity_dups = [d for d in opportunities['duplicate_code'] if d.get('similarity', 0) > 0.8]
            if high_similarity_dups:
                critical_issues.append(f"**–í—ã—Å–æ–∫–æ–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞** - {len(high_similarity_dups)} –±–ª–æ–∫–æ–≤ —Å similarity > 80%")
        
        if critical_issues:
            for issue in critical_issues:
                return f"1. {issue}\n"
        else:
            return "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.\n"

        return """

### –í–∞–∂–Ω—ã–µ (–≤–ª–∏—è—é—Ç –Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ—Å—Ç—å)
"""

        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–∂–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
        important_issues = []
        
        if opportunities.get('large_methods'):
            important_issues.append(f"**–ö—Ä—É–ø–Ω—ã–µ –º–µ—Ç–æ–¥—ã** - {len(opportunities['large_methods'])} –º–µ—Ç–æ–¥–æ–≤ —Ç—Ä–µ–±—É—é—Ç —Ä–∞–∑–±–∏–µ–Ω–∏—è")
        
        if opportunities.get('long_parameter_lists'):
            important_issues.append(f"**–î–ª–∏–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤** - {len(opportunities['long_parameter_lists'])} –º–µ—Ç–æ–¥–æ–≤")
        
        if important_issues:
            for i, issue in enumerate(important_issues, 1):
                return f"{i}. {issue}\n"
        else:
            return "–í–∞–∂–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.\n"

        return """

## üìã –î–µ—Ç–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π

### –§–∞–∑–∞ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ (1-2 –¥–Ω—è)
1. **–°–æ–∑–¥–∞–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤**
   - –ü–æ–∫—Ä—ã—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
   - –ó–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—É—â–µ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
   - –û–±–µ—Å–ø–µ—á–∏—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞

2. **–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π**
   - –í—ã—è–≤–∏—Ç—å –≤—Å–µ –≤–Ω–µ—à–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
   - –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–æ—á–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
   - –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –º–æ–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

### –§–∞–∑–∞ 2: –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (2-3 –¥–Ω—è)
"""

        if opportunities.get('duplicate_code'):
            for i, dup in enumerate(opportunities['duplicate_code'][:5], 1):
                return f"{i}. **–î—É–±–ª–∏–∫–∞—Ç {i}** (similarity: {dup.get('similarity', 0):.1%})\n"
                return f"   - –õ–æ–∫–∞—Ü–∏–∏: {len(dup.get('locations', []))}\n"
                return f"   - –î–µ–π—Å—Ç–≤–∏–µ: {dup.get('recommendation', '–ò–∑–≤–ª–µ—á—å –≤ –æ–±—â—É—é —Ñ—É–Ω–∫—Ü–∏—é')}\n"

        return """

### –§–∞–∑–∞ 3: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫—Ä—É–ø–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ (3-5 –¥–Ω–µ–π)
"""

        if opportunities.get('god_objects'):
            for god_obj in opportunities['god_objects']:
                return f"**–ö–ª–∞—Å—Å {god_obj['class']}:**\n"
                return f"- –ú–µ—Ç–æ–¥–æ–≤: {god_obj['method_count']}\n"
                return "- –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –í—ã–¥–µ–ª–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏\n"
                return f"- –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {god_obj.get('priority', 'HIGH')}\n\n"

        return """

### –§–∞–∑–∞ 4: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ (2-3 –¥–Ω—è)
"""

        if opportunities.get('large_methods'):
            for method in opportunities['large_methods'][:3]:
                return f"**–ú–µ—Ç–æ–¥ {method['function']}:**\n"
                return f"- –°—Ç—Ä–æ–∫: {method['lines']}\n"
                return f"- –î–µ–π—Å—Ç–≤–∏–µ: {method.get('recommendation', '–†–∞–∑–±–∏—Ç—å –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏')}\n\n"

        return """

## üîç –†–µ–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ß–∞—Å—Ç—ã–µ –≤—ã–∑–æ–≤—ã –º–µ—Ç–æ–¥–æ–≤
"""

        method_calls = usage_patterns.get('method_calls', [])
        if method_calls:
            call_counter = Counter(call.get('function_name') for call in method_calls if call.get('function_name'))
            for method, count in call_counter.most_common(5):
                return f"- **{method}**: {count} –≤—ã–∑–æ–≤–æ–≤\n"
        else:
            return "–î–∞–Ω–Ω—ã–µ –æ –≤—ã–∑–æ–≤–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.\n"

        return """

### –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
"""

        param_patterns = usage_patterns.get('parameter_patterns', {})
        common_params = param_patterns.get('common_parameters', {})
        if common_params:
            for param, count in common_params.most_common(10):
                real_values = param_patterns.get('real_values', {}).get(param, [])
                return f"- **{param}**: {count} –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π"
                if real_values:
                    return f" (–ø—Ä–∏–º–µ—Ä—ã: {', '.join(map(str, real_values[:3]))})\n"
                else:
                    return "\n"

        return f"""

## ‚ö†Ô∏è –û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤

### –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: {recommendations.get('risk_assessment', {}).get('complexity_risk', 'MEDIUM')}
### –†–∏—Å–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π: {recommendations.get('risk_assessment', {}).get('dependency_risk', 'MEDIUM')}
### –†–∏—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {recommendations.get('risk_assessment', {}).get('testing_risk', 'HIGH')}

### –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∏—Å–∫–æ–≤:
"""

        mitigation = recommendations.get('risk_assessment', {}).get('mitigation_strategies', [])
        for strategy in mitigation:
            return f"- {strategy}\n"

        return """

## üéØ –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞

### –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
- [ ] –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ü–∏–∫–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –Ω–∞ 30%
- [ ] –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞ –¥–æ <5%
- [ ] –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ God Objects (–º–µ—Ç–æ–¥–æ–≤ –≤ –∫–ª–∞—Å—Å–µ <15)
- [ ] –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –∫—Ä—É–ø–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ (<50 —Å—Ç—Ä–æ–∫)

### –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è
- [ ] –£–ª—É—á—à–µ–Ω–∏–µ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ –∫–æ–¥–∞
- [ ] –ü–æ–≤—ã—à–µ–Ω–∏–µ —Ç–µ—Å—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏
- [ ] –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ SOLID –ø—Ä–∏–Ω—Ü–∏–ø–∞–º
- [ ] –£–ø—Ä–æ—â–µ–Ω–∏–µ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏—è

## üìÖ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏

**–û–±—â–µ–µ –≤—Ä–µ–º—è:** 8-13 –¥–Ω–µ–π
- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞: 1-2 –¥–Ω—è
- –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: 2-3 –¥–Ω—è  
- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: 3-5 –¥–Ω–µ–π
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤: 2-3 –¥–Ω—è

## üîß –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ —Ç–µ—Ö–Ω–∏–∫–∏

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞
1. **Extract Method** - –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –∫—Ä—É–ø–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤
2. **Extract Class** - –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è God Objects
3. **Move Method** - –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–≤—è–∑–Ω–æ—Å—Ç–∏
4. **Replace Parameter with Object** - –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

### –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
- IntelliRefactor –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
- –•–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
- Code coverage –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ—Å—Ç–æ–≤

---
*–ü–ª–∞–Ω —Å–æ–∑–¥–∞–Ω –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞*
*–û—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ*
"""

    def save_optimized_results(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        self.logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ JSON
        results_path = self.output_dir / f"OPTIMIZED_REFACTORING_DATA_{self.timestamp}.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(self.refactoring_data, f, ensure_ascii=False, indent=2, default=str)
        
        self.analysis_results["generated_files"].append(str(results_path))
        
        # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç
        summary_path = self.output_dir / f"REFACTORING_SUMMARY_{self.timestamp}.md"
        summary_content = self._create_summary_report()
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(summary_content)
        
        self.analysis_results["generated_files"].append(str(summary_path))
        
        self.logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
        self.logger.info(f"–ö—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç: {summary_path}")

    def _create_summary_report(self) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç"""
        structure = self.refactoring_data.get('file_structure', {})
        opportunities = self.refactoring_data.get('refactoring_opportunities', {})
        
        return f"""# –ö—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞

**–§–∞–π–ª:** {self.target_file.name}
**–î–∞—Ç–∞:** {self.timestamp}

## –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Ö–æ–¥–∫–∏

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞
- –ö–ª–∞—Å—Å–æ–≤: {structure.get('complexity_metrics', {}).get('total_classes', 0)}
- –§—É–Ω–∫—Ü–∏–π: {structure.get('complexity_metrics', {}).get('total_functions', 0)}
- –°—Ç—Ä–æ–∫ –∫–æ–¥–∞: {structure.get('complexity_metrics', {}).get('lines_of_code', 0)}

### –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞
- God Objects: {len(opportunities.get('god_objects', []))}
- –ö—Ä—É–ø–Ω—ã–µ –º–µ—Ç–æ–¥—ã: {len(opportunities.get('large_methods', []))}
- –î—É–±–ª–∏–∫–∞—Ç—ã –∫–æ–¥–∞: {len(opportunities.get('duplicate_code', []))}
- –î–ª–∏–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {len(opportunities.get('long_parameter_lists', []))}

## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

1. **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1:** –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ God Objects
2. **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2:** –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∫–æ–¥–∞
3. **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3:** –†–∞–∑–±–∏–µ–Ω–∏–µ –∫—Ä—É–ø–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

1. –ò–∑—É—á–∏—Ç–µ –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω: `OPTIMIZED_REFACTORING_PLAN_{self.timestamp}.md`
2. –ü—Ä–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: `OPTIMIZED_REFACTORING_DATA_{self.timestamp}.json`
3. –ù–∞—á–Ω–∏—Ç–µ —Å —Å–æ–∑–¥–∞–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤

---
*–°–æ–∑–¥–∞–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞*
"""


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–ª–∞–Ω–∞ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python optimized_refactoring_analyzer.py /path/to/project /path/to/file.py /path/to/output
  python optimized_refactoring_analyzer.py C:\\Project C:\\Project\\module.py C:\\Results --verbose
        """,
    )

    parser.add_argument("project_path", help="–ü—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–µ –ø—Ä–æ–µ–∫—Ç–∞")
    parser.add_argument("target_file", help="–ü—É—Ç—å –∫ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–æ–º—É —Ñ–∞–π–ª—É")
    parser.add_argument("output_dir", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞")
    parser.add_argument("--verbose", "-v", action="store_true", help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∞–Ω–∞–ª–∏–∑–∞")

    args = parser.parse_args()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø—É—Ç–µ–π
    project_path = Path(args.project_path)
    target_file = Path(args.target_file)

    if not project_path.exists():
        print(f"–û—à–∏–±–∫–∞: –ü—Ä–æ–µ–∫—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {project_path}")
        sys.exit(1)

    if not target_file.exists():
        print(f"–û—à–∏–±–∫–∞: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {target_file}")
        sys.exit(1)

    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    try:
        analyzer = OptimizedRefactoringAnalyzer(
            str(project_path), str(target_file), args.output_dir, args.verbose
        )

        print("=" * 80)
        print("–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† –î–õ–Ø –†–ï–§–ê–ö–¢–û–†–ò–ù–ì–ê")
        print("=" * 80)
        print(f"–ü—Ä–æ–µ–∫—Ç: {project_path}")
        print(f"–§–∞–π–ª: {target_file}")
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {args.output_dir}")
        print("–°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ–µ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–ª–∞–Ω–∞ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞!")
        print("=" * 80)

        success = analyzer.run_optimized_analysis()

        if success:
            print("\n" + "=" * 80)
            print("‚úÖ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
            print("=" * 80)
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {args.output_dir}")
            print(f"–î–µ—Ç–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω: OPTIMIZED_REFACTORING_PLAN_{analyzer.timestamp}.md")
            print(f"–ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: OPTIMIZED_REFACTORING_DATA_{analyzer.timestamp}.json")
            print(f"–ö—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç: REFACTORING_SUMMARY_{analyzer.timestamp}.md")
            print("üéØ –ì–û–¢–û–í –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–ô –ü–õ–ê–ù –†–ï–§–ê–ö–¢–û–†–ò–ù–ì–ê!")
        else:
            print("\n" + "=" * 80)
            print("‚ö†Ô∏è –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù –° –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø–ú–ò")
            print("=" * 80)
            print(f"–ß–∞—Å—Ç–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤: {args.output_dir}")

        sys.exit(0 if success else 1)

    except KeyboardInterrupt:
        print("\n[–ü–†–ï–†–í–ê–ù–û] –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"\n[–û–®–ò–ë–ö–ê] –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()