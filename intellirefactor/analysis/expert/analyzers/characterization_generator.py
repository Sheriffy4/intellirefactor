"""
Characterization Test Generator for expert refactoring analysis.

Generates characterization tests (input â†’ output) to preserve
behavior during refactoring.
"""

from __future__ import annotations

import ast
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional

from ..models import CharacterizationTest, TestCategory

logger = logging.getLogger(__name__)


class CharacterizationTestGenerator:
    """Generates characterization tests for safe refactoring."""

    def __init__(self, project_root: str, target_module: str):
        self.project_root = Path(project_root)
        self.target_module = Path(target_module)

    def generate_characterization_tests(self, module_ast: ast.Module) -> List[CharacterizationTest]:
        """
        Generate characterization tests for the module.
        
        Args:
            module_ast: Parsed AST of the target module
            
        Returns:
            List of CharacterizationTest objects
        """
        logger.info("Generating characterization tests...")
        
        tests = []
        
        # Extract public methods and functions
        for node in ast.walk(module_ast):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                if not node.name.startswith('_'):  # Public functions
                    test_cases = self._generate_function_tests(node)
                    tests.extend(test_cases)
        
        logger.info(f"Generated {len(tests)} characterization tests")
        return tests

    def export_detailed_characterization_tests(self, tests: List[CharacterizationTest]) -> Dict[str, any]:
        """
        Export detailed characterization test data as requested by experts.
        
        Returns:
            Dictionary with executable test code and detailed test cases
        """
        # Group tests by method and category
        tests_by_method = {}
        tests_by_category = {"typical": [], "edge_case": [], "error_case": []}
        
        for test in tests:
            # Group by method
            if test.method_name not in tests_by_method:
                tests_by_method[test.method_name] = []
            tests_by_method[test.method_name].append(test)
            
            # Group by category
            category = test.test_category.value
            if category in tests_by_category:
                tests_by_category[category].append(test)
        
        # Generate executable test code
        test_code = self._generate_detailed_test_code(tests)
        
        # Create detailed test specifications
        detailed_tests = []
        for test in tests:
            detailed_test = {
                "method": test.method_name,
                "category": test.test_category.value,
                "priority": test.priority,
                "description": test.description,
                "input_params": test.input_params,
                "expected_behavior": self._describe_expected_behavior(test),
                "test_code": self._generate_single_test_code(test),
                "mocks_needed": self._identify_required_mocks(test),
                "invariants": self._identify_invariants(test)
            }
            detailed_tests.append(detailed_test)
        
        return {
            "characterization_tests": detailed_tests,
            "summary": {
                "total_tests": len(tests),
                "by_category": {
                    category: len(category_tests) 
                    for category, category_tests in tests_by_category.items()
                },
                "by_method": {
                    method: len(method_tests) 
                    for method, method_tests in tests_by_method.items()
                }
            },
            "executable_test_code": test_code,
            "test_file_path": f"test_characterization_{self.target_module.stem}.py",
            "setup_requirements": self._generate_setup_requirements(),
            "mock_specifications": self._generate_mock_specifications()
        }

    def _generate_detailed_test_code(self, tests: List[CharacterizationTest]) -> str:
        """Generate detailed Python test code with proper mocking and assertions."""
        lines = [
            '"""',
            'Characterization tests generated for safe refactoring.',
            'These tests capture current behavior to detect regressions.',
            '',
            'Generated by Expert Refactoring Analyzer',
            '"""',
            '',
            'import pytest',
            'from unittest.mock import Mock, patch, MagicMock',
            'import sys',
            'from pathlib import Path',
            '',
            '# Add project root to path',
            'project_root = Path(__file__).parent',
            'sys.path.insert(0, str(project_root))',
            '',
            '# Import target module',
            'try:',
            f'    from {self.target_module.stem} import *',
            'except ImportError as e:',
            '    pytest.skip(f"Could not import target module: {e}")',
            '',
            '',
        ]
        
        # Group tests by method
        tests_by_method = {}
        for test in tests:
            if test.method_name not in tests_by_method:
                tests_by_method[test.method_name] = []
            tests_by_method[test.method_name].append(test)
        
        # Generate test classes for each method
        for method_name, method_tests in tests_by_method.items():
            lines.extend([
                f'class Test{method_name.title().replace("_", "")}:',
                f'    """Characterization tests for {method_name}."""',
                '',
                '    def setup_method(self):',
                '        """Set up test fixtures."""',
                '        # TODO: Initialize required objects and mocks',
                '        pass',
                '',
            ])
            
            # Generate individual test methods
            for i, test in enumerate(method_tests):
                test_method_name = f"test_{method_name}_{test.test_category.value}_{i+1}"
                
                lines.extend([
                    f'    def {test_method_name}(self):',
                    '        """',
                    f'        Test: {test.description}',
                    f'        Category: {test.test_category.value}',
                    f'        Priority: {test.priority}',
                    '        """',
                ])
                
                # Add mock setup if needed
                mocks_needed = self._identify_required_mocks(test)
                if mocks_needed:
                    lines.append('        # Set up mocks')
                    for mock_name in mocks_needed:
                        lines.append(f'        {mock_name}_mock = Mock()')
                    lines.append('')
                
                # Add test execution
                lines.extend([
                    '        # Execute the method under test',
                    '        try:',
                ])
                
                if test.input_params:
                    param_str = ', '.join([f'{k}={repr(v)}' for k, v in test.input_params.items()])
                    lines.append(f'            result = {method_name}({param_str})')
                else:
                    lines.append(f'            result = {method_name}()')
                
                lines.extend([
                    '            ',
                    '            # TODO: Capture and verify the actual result',
                    '            # assert result == expected_result',
                    '            # For now, just ensure no exceptions are raised',
                    '            assert True  # Placeholder assertion',
                    '            ',
                ])
                
                # Add error case handling
                if test.test_category.value == "error_case":
                    lines.extend([
                        '        except Exception as e:',
                        '            # TODO: Verify this is the expected exception type',
                        '            # assert isinstance(e, ExpectedExceptionType)',
                        '            pass  # Expected error case',
                    ])
                else:
                    lines.extend([
                        '        except Exception as e:',
                        '            pytest.fail(f"Unexpected exception: {e}")',
                    ])
                
                lines.extend(['', ''])
            
            lines.append('')
        
        # Add utility methods
        lines.extend([
            '',
            '# Utility functions for test setup',
            '',
            'def create_mock_attack_registry():',
            '    """Create a mock AttackRegistry for testing."""',
            '    mock_registry = Mock()',
            '    # TODO: Configure mock behavior based on actual usage',
            '    return mock_registry',
            '',
            'def create_mock_parameter_normalizer():',
            '    """Create a mock ParameterNormalizer for testing."""',
            '    mock_normalizer = Mock()',
            '    # TODO: Configure mock behavior based on actual usage',
            '    return mock_normalizer',
            '',
            'def create_mock_sni_extractor():',
            '    """Create a mock SNIExtractor for testing."""',
            '    mock_extractor = Mock()',
            '    # TODO: Configure mock behavior based on actual usage',
            '    return mock_extractor',
            '',
        ])
        
        return '\n'.join(lines)

    def _generate_single_test_code(self, test: CharacterizationTest) -> str:
        """Generate code for a single test case."""
        lines = []
        
        # Method call
        if test.input_params:
            param_str = ', '.join([f'{k}={repr(v)}' for k, v in test.input_params.items()])
            lines.append(f'result = {test.method_name}({param_str})')
        else:
            lines.append(f'result = {test.method_name}()')
        
        # Basic assertion
        lines.append('assert result is not None  # TODO: Add specific assertion')
        
        return '\n'.join(lines)

    def _describe_expected_behavior(self, test: CharacterizationTest) -> str:
        """Describe the expected behavior for a test case."""
        if test.test_category.value == "typical":
            return f"Should execute {test.method_name} successfully with typical parameters"
        elif test.test_category.value == "edge_case":
            return "Should handle edge case parameters gracefully"
        elif test.test_category.value == "error_case":
            return "Should raise appropriate exception for invalid parameters"
        else:
            return f"Should behave correctly for {test.test_category.value} scenario"

    def _identify_required_mocks(self, test: CharacterizationTest) -> List[str]:
        """Identify what mocks are needed for this test."""
        # Based on common patterns in the target module
        common_mocks = []
        
        method_name = test.method_name.lower()
        if 'dispatch' in method_name or 'attack' in method_name:
            common_mocks.extend(['AttackRegistry', 'ParameterNormalizer'])
        
        if 'sni' in method_name or 'parse' in method_name:
            common_mocks.append('SNIExtractor')
        
        if 'log' in method_name:
            common_mocks.append('operation_logger')
        
        return common_mocks

    def _identify_invariants(self, test: CharacterizationTest) -> List[str]:
        """Identify invariants that should be preserved."""
        invariants = []
        
        method_name = test.method_name.lower()
        if 'dispatch' in method_name:
            invariants.extend([
                "Attack recipe structure should be preserved",
                "Parameter validation should occur before execution",
                "Logging should capture all operations"
            ])
        
        if 'normalize' in method_name:
            invariants.extend([
                "Input parameters should be validated",
                "Output format should be consistent"
            ])
        
        return invariants

    def _generate_setup_requirements(self) -> List[str]:
        """Generate setup requirements for the tests."""
        return [
            "pytest >= 6.0.0",
            "unittest.mock (built-in)",
            "Target module must be importable",
            "Mock objects for external dependencies",
            "Test data fixtures if needed"
        ]

    def _generate_mock_specifications(self) -> Dict[str, Dict[str, str]]:
        """Generate specifications for required mocks."""
        return {
            "AttackRegistry": {
                "description": "Mock for attack registry containing attack handlers",
                "key_methods": ["get_attack", "list_attacks", "validate_attack"],
                "return_types": "Dict[str, AttackHandler]"
            },
            "ParameterNormalizer": {
                "description": "Mock for parameter normalization and validation",
                "key_methods": ["normalize", "validate"],
                "return_types": "Dict[str, Any]"
            },
            "SNIExtractor": {
                "description": "Mock for SNI extraction from TLS packets",
                "key_methods": ["extract_sni", "is_tls_packet"],
                "return_types": "Optional[str]"
            }
        }

    def _generate_function_tests(self, func_node: ast.FunctionDef) -> List[CharacterizationTest]:
        """Generate test cases for a single function."""
        tests = []
        
        # Get function signature
        params = self._extract_parameters(func_node)
        
        # Generate different categories of tests
        tests.extend(self._generate_typical_tests(func_node, params))
        tests.extend(self._generate_edge_case_tests(func_node, params))
        tests.extend(self._generate_error_tests(func_node, params))
        
        return tests

    def _extract_parameters(self, func_node: ast.FunctionDef) -> List[Dict[str, Any]]:
        """Extract parameter information from function."""
        params = []
        
        if func_node.args:
            for arg in func_node.args.args:
                param_info = {
                    'name': arg.arg,
                    'type': None,
                    'default': None
                }
                
                # Try to get type annotation
                if arg.annotation:
                    try:
                        if hasattr(ast, 'unparse'):
                            param_info['type'] = ast.unparse(arg.annotation)
                    except Exception:
                        pass
                
                params.append(param_info)
        
        return params

    def _generate_typical_tests(self, func_node: ast.FunctionDef, params: List[Dict[str, Any]]) -> List[CharacterizationTest]:
        """Generate typical use case tests."""
        tests = []
        
        # Generate a basic test case
        test_params = {}
        for param in params:
            if param['name'] == 'self':
                continue
            test_params[param['name']] = self._generate_typical_value(param)
        
        test = CharacterizationTest(
            method_name=func_node.name,
            input_params=test_params,
            test_category=TestCategory.TYPICAL,
            priority=1,
            description=f"Typical usage of {func_node.name}"
        )
        tests.append(test)
        
        return tests

    def _generate_edge_case_tests(self, func_node: ast.FunctionDef, params: List[Dict[str, Any]]) -> List[CharacterizationTest]:
        """Generate edge case tests."""
        tests = []
        
        # Generate edge cases for each parameter
        for param in params:
            if param['name'] == 'self':
                continue
            
            edge_values = self._generate_edge_values(param)
            for edge_value in edge_values:
                test_params = {}
                for p in params:
                    if p['name'] == 'self':
                        continue
                    if p['name'] == param['name']:
                        test_params[p['name']] = edge_value
                    else:
                        test_params[p['name']] = self._generate_typical_value(p)
                
                test = CharacterizationTest(
                    method_name=func_node.name,
                    input_params=test_params,
                    test_category=TestCategory.EDGE_CASE,
                    priority=2,
                    description=f"Edge case: {param['name']} = {edge_value}"
                )
                tests.append(test)
        
        return tests

    def _generate_error_tests(self, func_node: ast.FunctionDef, params: List[Dict[str, Any]]) -> List[CharacterizationTest]:
        """Generate error condition tests."""
        tests = []
        
        # Generate error cases
        for param in params:
            if param['name'] == 'self':
                continue
            
            error_values = self._generate_error_values(param)
            for error_value in error_values:
                test_params = {}
                for p in params:
                    if p['name'] == 'self':
                        continue
                    if p['name'] == param['name']:
                        test_params[p['name']] = error_value
                    else:
                        test_params[p['name']] = self._generate_typical_value(p)
                
                test = CharacterizationTest(
                    method_name=func_node.name,
                    input_params=test_params,
                    test_category=TestCategory.ERROR_CASE,
                    priority=3,
                    description=f"Error case: {param['name']} = {error_value}"
                )
                tests.append(test)
        
        return tests

    def _generate_typical_value(self, param: Dict[str, Any]) -> Any:
        """Generate a typical value for a parameter."""
        param_type = param.get('type', '').lower()
        
        if 'str' in param_type:
            return "test_string"
        elif 'int' in param_type:
            return 42
        elif 'float' in param_type:
            return 3.14
        elif 'bool' in param_type:
            return True
        elif 'list' in param_type:
            return [1, 2, 3]
        elif 'dict' in param_type:
            return {"key": "value"}
        else:
            # Default based on parameter name
            name = param['name'].lower()
            if 'count' in name or 'size' in name or 'length' in name:
                return 10
            elif 'name' in name or 'text' in name or 'message' in name:
                return "test_value"
            elif 'flag' in name or 'enabled' in name:
                return True
            else:
                return None

    def _generate_edge_values(self, param: Dict[str, Any]) -> List[Any]:
        """Generate edge case values for a parameter."""
        param_type = param.get('type', '').lower()
        
        if 'str' in param_type:
            return ["", "a", "very_long_string_" * 10]
        elif 'int' in param_type:
            return [0, -1, 1, 999999, -999999]
        elif 'float' in param_type:
            return [0.0, -1.0, 1.0, float('inf'), float('-inf')]
        elif 'bool' in param_type:
            return [False]  # True is typical
        elif 'list' in param_type:
            return [[], [1], list(range(1000))]
        elif 'dict' in param_type:
            return [{}, {"single": "item"}, {f"key_{i}": f"value_{i}" for i in range(100)}]
        else:
            return [None, 0, "", []]

    def _generate_error_values(self, param: Dict[str, Any]) -> List[Any]:
        """Generate values that might cause errors."""
        param_type = param.get('type', '').lower()
        
        error_values = [None]  # None is often an error case
        
        if 'str' in param_type:
            error_values.extend([123, [], {}])  # Wrong types
        elif 'int' in param_type:
            error_values.extend(["not_a_number", [], {}])
        elif 'list' in param_type:
            error_values.extend(["not_a_list", 123])
        elif 'dict' in param_type:
            error_values.extend(["not_a_dict", 123, []])
        
        return error_values

    def create_executable_tests(self, tests: List[CharacterizationTest], output_file: Optional[str] = None) -> str:
        """
        Create executable pytest-compatible test file.
        
        Args:
            tests: List of characterization tests
            output_file: Optional output file path
            
        Returns:
            Path to generated test file
        """
        if not output_file:
            output_file = f"test_characterization_{self.target_module.stem}.py"
        
        test_code = self._generate_test_code(tests)
        
        output_path = self.project_root / output_file
        output_path.write_text(test_code, encoding='utf-8')
        
        logger.info(f"Generated executable tests: {output_path}")
        return str(output_path)

    def _generate_test_code(self, tests: List[CharacterizationTest]) -> str:
        """Generate Python test code from characterization tests."""
        lines = [
            "\"\"\"",
            "Characterization tests generated for safe refactoring.",
            "These tests capture current behavior to detect regressions.",
            "\"\"\"",
            "",
            "import pytest",
            f"from {self.target_module.stem} import *",
            "",
            "",
        ]
        
        # Group tests by method
        tests_by_method = {}
        for test in tests:
            if test.method_name not in tests_by_method:
                tests_by_method[test.method_name] = []
            tests_by_method[test.method_name].append(test)
        
        # Generate test functions
        for method_name, method_tests in tests_by_method.items():
            lines.extend([
                f"class Test{method_name.title()}:",
                f'    """Characterization tests for {method_name}."""',
                "",
            ])
            
            for i, test in enumerate(method_tests):
                test_name = f"test_{method_name}_{test.test_category.value}_{i}"
                lines.extend([
                    f"    def {test_name}(self):",
                    f'        """Test: {test.description}"""',
                    "        # TODO: Capture actual output and add assertion",
                    f"        result = {method_name}(",
                ])
                
                # Add parameters
                for param_name, param_value in test.input_params.items():
                    lines.append(f"            {param_name}={repr(param_value)},")
                
                lines.extend([
                    "        )",
                    "        # assert result == expected_output  # TODO: Add expected output",
                    "",
                ])
            
            lines.append("")
        
        return "\n".join(lines)