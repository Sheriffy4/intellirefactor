Ниже два артефакта:

1) **План для программиста (эксперта)** — как тимлид: что открыть, какие файлы/артефакты использовать, в каком порядке выполнять дедуп/рефакторинг/декомпозицию, какие критерии готовности и проверки на каждом шаге.  
2) **План‑промпт для LLM** — чтобы LLM мог выполнять ту же задачу (с чётким входом/выходом, правилами и запретами).

Оба плана поддерживают **2 режима**:
- **Project mode**: сначала по всему проекту → топ‑N keystone → работа итерациями по файлам/модулям.  
- **Target mode**: ты явно задаёшь конкретный файл → делаем план и изменения вокруг него.

---

# 1) План для программиста (эксперта) — “Refactor + Decompose + Dedup”

## 0. Роли и цель
**Цель:** снизить риск/сложность/дубли, улучшить архитектурные границы, при этом сохранить поведение.

**Принцип:** “Сначала уменьшаем повторение и риск (dedup), потом упрощаем (refactor), потом режем по границам (decompose), после каждого крупного шага — пересчёт индекса/аудита”.

---

## 1) Подготовка окружения и данных (обязательно)

### 1.1. Выбрать режим
- **Project mode**: делаем план на весь проект.
- **Target mode**: делаем план вокруг одного файла/модуля.

### 1.2. Обязательные входные артефакты (источник истины)
Работаем по последнему run-dir (например `intellirefactor_out/<run_id>`):

1) **Источник истины:** `audit/audit.json`  
   Взять:
   - `findings[]`:
     - `finding_type`
     - `severity`
     - `confidence`
     - `file_path`
     - `line_start`, `line_end`
     - `metadata` (clone_type, smell_type, symbol_name, etc.)
   - `statistics.findings_by_type`, `statistics.findings_by_severity`
   - `unused_result` (если есть) — чтобы не удалить используемое
   - `clone_groups` (если есть) — для конкретной дедупликации

2) **Index derived:**  
   - `dashboard/dependency_hubs.json`:
     - `top_files_by_fanout` (fanout/fanin/keystone_score, hotspot_score_norm)
     - `top_external_targets` (что чаще всего дергается)
   - `dashboard/dependency_graph.json`:
     - `nodes[]`, `edges[]`, `summary`
     - найти, какие файлы реально зависят от target
   - `index/status.json` (если есть) — для понимания масштаба

3) **Dedup artifacts (для тикетов и конкретики):**
   - `dedup/block_clones.json` + `dedup/block_clones.csv`  
   - (если включено) `dedup/block_clones_full.json` — чтобы увидеть сниппеты
   - `dedup/index_duplicates.json` — дубли по базе (полезно для cross‑file)

4) **Refactor artifacts:**
   - `refactor/unused.json` (если есть)
   - `refactor/project_analysis.json`, `refactor/file_analysis.json` (если target был)

### 1.3. Обязательная проверка корректности данных (до начала работ)
Запуск:
```bash
python intellirefactor/tools/check_db_schema_live.py --db <project>/.intellirefactor/index.db --run-dir <run_dir>
```
- Если есть **ошибки DB / dashboard** — сначала исправить (иначе план будет на “битых” данных).

---

## 2) Быстрый обзор состояния проекта (Project mode)

### 2.1. Снять “карту приоритетов”
Открыть и зафиксировать:
- `dashboard/health_score.json` — baseline
- `dashboard/hotspots.json` — топ‑25 проблемных файлов по findings
- `dashboard/dependency_hubs.json` — топ keystone файлов (fanout * hotspot_norm)
- `audit/audit.json` — распределение проблем по типам и severity

### 2.2. Определить **Top‑5 кандидатов** на работу (keystone + hotspots)
Правило выбора:
- берем пересечение/объединение:
  - Top‑5 `dependency_hubs.top_files_by_fanout` по `keystone_score`
  - Top‑5 `hotspots.hotspots` по `score`
- убираем очевидные “архив/скрипты/одноразовое” если они не в core (по решению тимлида)

Результат: список `candidate_files = [F1..F5]`.

---

## 3) Анализ “целевого файла” (Target mode) — для каждого кандидата или для вручную выбранного

Для файла `TARGET`:

### 3.1. Собрать “паспорт цели”
Из `hotspots.json` найти запись по `file_path` (suffix match допустим) и выписать:
- hotspot score, findings_total
- by_type (duplicate_block / unused_code / quality_issue)
- by_severity

Из `dependency_hubs.json` найти строку по `file_path`:
- `deps_total` (fanout)
- `fanin_total` (понимая, что он может быть занижен)
- `keystone_score`

Из `dependency_graph.json`:
- найти node `file_path == TARGET`
- собрать:
  - исходящие/входящие ребра
  - топ зависимостей (какие модули/файлы связаны)

Из `audit/audit.json`:
- собрать все findings по TARGET (match пути)
- сгруппировать по:
  - типам (duplicate/unused/quality)
  - “кластеру” по линиям (пересечения диапазонов line_start/line_end)

### 3.2. Определить стратегию для TARGET
- Если в TARGET много `duplicate_block` → **dedup-first**
- Если TARGET keystone (высокий keystone_score) → **рефакторинг в малых шагах + тесты**
- Если TARGET часть цикла / сильной связанности → **decomposition plan** после упрощения

---

## 4) Порядок работ (общий алгоритм)

### 4.1. Итерация “Dedup → Refactor → Decompose → Re-audit”
Для каждого TARGET делаем:

#### Шаг A: Dedup (подготовка)
**Цель:** сократить повторение, снизить объем изменений дальше.

1) Открыть `dedup/block_clones.csv` и отфильтровать группы, где TARGET участвует:
   - group_id
   - clone_type
   - instance_count
   - unique_files
   - top_location
2) Для топ‑групп (обычно 3–10) открыть:
   - `dedup/block_clones_full.json` (если есть) и посмотреть сниппеты
   - или вручную открыть участки кода по координатам
3) Для каждой группы определить стратегию:
   - extract_method / extract_function / extract_class / parameterize  
   (ориентируясь на clone_type, LOC, контекст)
4) Реализация:
   - выбрать “каноническую” реализацию
   - создать новый модуль/утилиту (например `validators.py`, `utils/path.py`, `common/logging.py`)
   - заменить копипасты на вызов
   - **обновить импорты**
5) Проверки:
   - прогнать тесты (если есть) или хотя бы `python -m py_compile` на изменённых файлах
   - минимальный smoke-run CLI (если применимо)

**Результат шага A:** уменьшение duplicated LOC и снижение риска последующего рефакторинга.

#### Шаг B: Refactor (локальное упрощение)
**Цель:** снизить сложность/размер/ветвления, сделать код модульным.

1) Открыть findings по TARGET типа `quality_issue` и high severity.
2) Если есть большие функции/классы:
   - выделить подфункции (“extract method”)
   - упростить условия (guard clauses)
   - убрать лишние responsibility (SRP)
3) Согласовать публичный API:
   - не ломать внешние вызовы (смотрим fanin / external usage)
   - если ломаем — делаем адаптер/compat слой
4) Если есть `unused_code` findings:
   - **сверить с `unused_result`**: убедиться что символ реально не используется
   - удалить/депрекейтить
5) Проверки:
   - линтер/форматтер (если есть в проекте)
   - тесты / smoke run

**Результат шага B:** TARGET стал проще и безопаснее для декомпозиции.

#### Шаг C: Decompose (архитектурный шаг)
**Цель:** разделить модули/пакеты по ответственностям, снизить coupling.

1) Открыть:
   - `decompose/smells.json` (если есть) и выбрать запахи, относящиеся к TARGET и связанным файлам
   - `dependency_graph.json` — какие связки надо “разрезать”
2) Определить границы компонентов:
   - выделить подмодуль/пакет (например `intellirefactor/analysis/index/` уже пакет — можно внутри разделить store/query/builder/api)
3) План миграции:
   - создать новые файлы/пакеты
   - переместить функции/классы
   - обновить импорты
   - оставить “реэкспорт” в старом месте на 1 итерацию (если внешний API)
4) Проверки:
   - тесты
   - smoke run
   - убедиться, что dependency_graph стал менее плотным вокруг TARGET (после пересчёта)

#### Шаг D: Пересчёт индекса/аудита (feedback loop)
После завершения по одному TARGET (или после 2–3 TARGET):
- прогнать `collect` снова (audit+index)
- сравнить:
  - `health_score`
  - `hotspots` (score TARGET должен упасть)
  - `dependency_hubs` (keystone_score/ fanout может измениться)
  - количество clone groups

---

## 5) Как сочетать Project mode и Target mode (как ты хочешь)

### Вариант 1 (рекомендуемый): Project → Top‑5 → Target итерации
1) Project mode: найти Top‑5 keystone/hotspots
2) Для каждого файла сделать Target mode план и выполнить A→B→C→D
3) После каждого файла пересчитывать audit/index (или каждые 2 файла)

Смысл есть: ты каждый раз видишь как изменился рейтинг и приоритеты.

### Вариант 2: “У меня подозрение на файл, но он не в топе”
1) Запускаешь Target mode на этом файле
2) Смотришь:
   - входит ли файл в dependency_graph как важный хаб (может он важен, но findings мало)
   - какие входящие (fanin) и внешние вызовы
3) Если файл действительно “узловой”, он может быть кандидатом даже без hotspots.

---

## 6) Definition of Done (критерии завершения)

Для каждого TARGET:
- Clone groups, где TARGET участвовал, уменьшились или исчезли
- число findings по TARGET снизилось (audit)
- build/smoke-run проходит
- dependency_graph:
  - меньше “тяжёлых” ребер или более очевидные границы (если делали decomposition)
- `check_db_schema_live.py` без ошибок

---

# 2) План‑промпт для LLM (аналогичная задача)

Ниже текст, который можно копировать в LLM. Он задаёт входные данные, строгий формат выхода и правила.

---

## PROMPT (LLM)

Ты — тимлид/архитектор. Нужно составить иерархический план работ (refactor + dedup + decomposition) по проекту IntelliRefactor на основе артефактов анализа.  
Работай строго по данным артефактов. Никаких выдуманных файлов/метрик.

### Входные данные
Тебе предоставлены артефакты одного run-dir (пути относительные run-dir):
1) `audit/audit.json` — источник истины
2) `dashboard/hotspots.json`
3) `dashboard/dependency_hubs.json`
4) `dashboard/dependency_graph.json`
5) `refactor/unused.json` (если есть)
6) `dedup/block_clones.json` и `dedup/block_clones.csv` (если есть)
7) `decompose/smells.json` (если есть)
8) `index/status.json` (если есть)

Также может быть дан параметр:
- `MODE = "project"` или `MODE = "target"`
- если `MODE="target"`, будет указан `TARGET_FILE` (путь как в hotspots/file_path или suffix).

### Задача
1) В режиме `project`:
   - определить Top‑5 файлов для работы, используя синергию:
     - hotspots score
     - keystone_score из dependency_hubs
     - данные о дубликатах (clone_groups или block_clones)
   - для каждого из Top‑5 сформировать отдельный план “Dedup → Refactor → Decompose → Re-audit”.

2) В режиме `target`:
   - построить план вокруг `TARGET_FILE`:
     - перечислить конкретные findings в target (тип/severity/confidence/линии/metadata)
     - перечислить relevant clone groups (где target участвует) и предложить dedup шаги
     - учесть `unused_result` чтобы не удалить используемое
     - учесть dependency_graph для оценки impact (какие файлы зависят/на что зависит)

### Правила (строго)
- Всегда ссылаться на конкретные поля из артефактов:  
  `findings[]` (finding_type, severity, confidence, file_path, line_start/line_end, metadata),  
  `statistics.findings_by_type`, `statistics.findings_by_severity`,  
  `unused_result`, `clone_groups`,  
  `dependency_hubs.top_files_by_fanout`,  
  `dependency_graph.nodes/edges/summary`.
- Если артефакт отсутствует — явно написать “missing” и предложить альтернативу.
- Для каждого шага указывать:
  - цель
  - входные данные (какие артефакты/поля используем)
  - действия
  - критерии проверки (какие команды/артефакты должны подтвердить успех)
  - ожидаемый эффект (что должно уменьшиться/измениться)

### Формат вывода (строго)
Выведи JSON с ключами:

```json
{
  "mode": "project|target",
  "inputs_used": {
    "run_dir": "...",
    "target_file": "... or null",
    "artifacts": { "audit": true/false, "hotspots": ..., "dependency_hubs": ..., "dependency_graph": ..., "dedup": ..., "decompose": ..., "unused": ... }
  },
  "project_overview": {
    "health_score": "... if available",
    "findings_summary": { "by_type": {}, "by_severity": {} },
    "top_hotspots": [ ... ],
    "top_keystone": [ ... ]
  },
  "work_plan": [
    {
      "target_file": "...",
      "rationale": [ "why chosen (hotspot/keystone/duplicates)" ],
      "passport": {
        "hotspot": { ... },
        "hubs": { ... },
        "dependency_graph_context": { "top_outgoing": [], "top_incoming": [] },
        "findings_in_file": [ ... ],
        "clone_groups_involving_file": [ ... ],
        "unused_in_file": [ ... ]
      },
      "steps": [
        { "phase": "dedup", "actions": [...], "checks": [...], "expected_effect": [...] },
        { "phase": "refactor", "actions": [...], "checks": [...], "expected_effect": [...] },
        { "phase": "decompose", "actions": [...], "checks": [...], "expected_effect": [...] },
        { "phase": "re-audit", "actions": [...], "checks": [...], "expected_effect": [...] }
      ]
    }
  ],
  "global_checks": [
    "Run check_db_schema_live.py ...",
    "Rebuild index ...",
    "Compare dashboards ..."
  ]
}
```

Если MODE=target — `work_plan` должен содержать один элемент по TARGET.

---

# 3) Важное уточнение по твоему сценарию “топ‑5, потом каждый отдельно прогоняю”
Да, смысл есть, и это хороший workflow:

- Первый прогон (project mode) даёт **приоритизацию** (keystone/hotspots/duplicates).
- Дальше ты берёшь каждый из топ‑5 и делаешь **target mode**:
  - потому что в target mode ты подтягиваешь конкретные clone groups / unused / edges вокруг файла,
  - и строишь пошаговый план именно для него.
- После 1–2 файлов пересчитываешь индекс/аудит — **приоритеты могут смениться**, и “топ‑5” обновится.

---

Если ты хочешь, я могу:
- добавить в `check_db_schema_live.py` те 3 смысловые проверки для dashboard (hotspots ↔ graph ↔ db),
- и/или сделать генератор “project refactoring path.json” (машиночитаемый план), который будет тем самым выходом для LLM/программиста.