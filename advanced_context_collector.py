#!/usr/bin/env python3
"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–±–æ—Ä—â–∏–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –º–æ–¥—É–ª–µ–π.

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –≤—ã–±–æ—Ä–∞ —Ñ–∞–π–ª–æ–≤
- –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –≠–∫—Å–ø–æ—Ä—Ç –≤ —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
- –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
"""

import json
from pathlib import Path
from typing import List, Dict, Tuple
from dataclasses import asdict
import argparse
from context_collector import ContextCollector, ContextFile


class AdvancedContextCollector(ContextCollector):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–±–æ—Ä—â–∏–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏."""
    
    def __init__(self, analysis_results_dir: str, target_module_path: str):
        super().__init__(analysis_results_dir, target_module_path)
        self.quality_metrics = {}
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        self.file_patterns['expert'] = [
            r"expert_analysis_report_\d{8}_\d{6}\.md",
            r"expert_analysis_\d{8}_\d{6}\.json",
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è structured ultimate analyzer —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.file_patterns['structured'] = [
            r"canonical_analysis_snapshot_\d{8}_\d{6}\.json",
            r"contextual_decision_analysis_\d{8}_\d{6}\.json",
            r"contextual_refactoring_decisions_\d{8}_\d{6}\.json",
        ]
    
    def collect_context_files(self) -> List[ContextFile]:
        """–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–∞–∑–º–µ—Ä–∞."""
        context_files = []
        
        # –≠–ö–°–ü–ï–†–¢–ù–´–ô –ê–ù–ê–õ–ò–ó (–Ω–∞–∏–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        for pattern in self.file_patterns['expert']:
            files = self.find_files_by_pattern(pattern, self.analysis_dir)
            for file_path in files:
                context_files.append(ContextFile(
                    path=str(file_path),
                    priority=0,  # –ù–∞–∏–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                    description=f"Expert refactoring analysis: {file_path.name}",
                    estimated_lines=self.get_file_size_estimate(file_path),
                    file_type='expert'
                ))
        
        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –¥–∏–∞–≥—Ä–∞–º–º—ã –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (–≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        for pattern in self.file_patterns['architecture']:
            files = self.find_files_by_pattern(pattern, self.analysis_dir)
            for file_path in files:
                context_files.append(ContextFile(
                    path=str(file_path),
                    priority=1,
                    description=f"Architecture analysis: {file_path.name}",
                    estimated_lines=self.get_file_size_estimate(file_path),
                    file_type='architecture'
                ))
        
        # –ü–ª–∞–Ω—ã —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ (–≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        for pattern in self.file_patterns['plan']:
            files = self.find_files_by_pattern(pattern, self.analysis_dir)
            for file_path in files:
                context_files.append(ContextFile(
                    path=str(file_path),
                    priority=1,
                    description=f"Refactoring plan: {file_path.name}",
                    estimated_lines=self.get_file_size_estimate(file_path),
                    file_type='plan'
                ))
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM (–≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        for pattern in self.file_patterns['context']:
            files = self.find_files_by_pattern(pattern, self.analysis_dir)
            for file_path in files:
                context_files.append(ContextFile(
                    path=str(file_path),
                    priority=1,
                    description=f"LLM context: {file_path.name}",
                    estimated_lines=self.get_file_size_estimate(file_path),
                    file_type='context'
                ))
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π —Ä–∞–∑–º–µ—Ä)
        for pattern in self.file_patterns['structured']:
            files = self.find_files_by_pattern(pattern, self.analysis_dir)
            for file_path in files:
                # –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä structured —Ñ–∞–π–ª–æ–≤
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
                if file_size_mb > 10:  # –ë–æ–ª—å—à–µ 10 –ú–ë - –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 200 —Å—Ç—Ä–æ–∫
                    estimated_lines = 200
                elif file_size_mb > 5:  # –ë–æ–ª—å—à–µ 5 –ú–ë - –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 300 —Å—Ç—Ä–æ–∫
                    estimated_lines = 300
                elif file_size_mb > 1:  # –ë–æ–ª—å—à–µ 1 –ú–ë - –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 500 —Å—Ç—Ä–æ–∫
                    estimated_lines = 500
                else:
                    estimated_lines = min(800, self.get_file_size_estimate(file_path))
                
                context_files.append(ContextFile(
                    path=str(file_path),
                    priority=1,
                    description=f"Structured analysis (first {estimated_lines} lines): {file_path.name}",
                    estimated_lines=estimated_lines,
                    file_type='structured'
                ))
        
        # –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ (—Å—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        for pattern in self.file_patterns['opportunities']:
            files = self.find_files_by_pattern(pattern, self.analysis_dir)
            for file_path in files:
                context_files.append(ContextFile(
                    path=str(file_path),
                    priority=2,
                    description=f"Refactoring opportunities: {file_path.name}",
                    estimated_lines=self.get_file_size_estimate(file_path),
                    file_type='opportunities'
                ))
        
        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –∑–∞–ø–∞—Ö–∏ (—Å—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç, —Å—Ç—Ä–æ–≥–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞)
        for pattern in self.file_patterns['smells']:
            files = self.find_files_by_pattern(pattern, self.analysis_dir)
            for file_path in files:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
                if file_size_mb > 5:  # –ë–æ–ª—å—à–µ 5 –ú–ë - –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 100 —Å—Ç—Ä–æ–∫
                    estimated_lines = 100
                elif file_size_mb > 1:  # –ë–æ–ª—å—à–µ 1 –ú–ë - –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 200 —Å—Ç—Ä–æ–∫
                    estimated_lines = 200
                else:
                    estimated_lines = min(300, self.get_file_size_estimate(file_path))
                
                context_files.append(ContextFile(
                    path=str(file_path),
                    priority=2,
                    description=f"Architectural smells (first {estimated_lines} lines): {file_path.name}",
                    estimated_lines=estimated_lines,
                    file_type='smells'
                ))
        
        # –î—É–±–ª–∏–∫–∞—Ç—ã –∫–æ–¥–∞ (—Å—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç, —Å—Ç—Ä–æ–≥–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞)
        for pattern in self.file_patterns['duplicates']:
            files = self.find_files_by_pattern(pattern, self.analysis_dir)
            for file_path in files:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
                if file_size_mb > 5:  # –ë–æ–ª—å—à–µ 5 –ú–ë - –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 50 —Å—Ç—Ä–æ–∫
                    estimated_lines = 50
                elif file_size_mb > 1:  # –ë–æ–ª—å—à–µ 1 –ú–ë - –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 100 —Å—Ç—Ä–æ–∫
                    estimated_lines = 100
                else:
                    estimated_lines = min(200, self.get_file_size_estimate(file_path))
                
                context_files.append(ContextFile(
                    path=str(file_path),
                    priority=3,
                    description=f"Code duplicates (first {estimated_lines} lines): {file_path.name}",
                    estimated_lines=estimated_lines,
                    file_type='duplicates'
                ))
        
        # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (–Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        docs_dir = self.analysis_dir / "docs"
        for pattern in self.file_patterns['docs']:
            files = self.find_files_by_pattern(pattern, docs_dir)
            for file_path in files:
                context_files.append(ContextFile(
                    path=str(file_path),
                    priority=4,
                    description=f"Documentation: {file_path.name}",
                    estimated_lines=self.get_file_size_estimate(file_path),
                    file_type='docs'
                ))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ —Ä–∞–∑–º–µ—Ä—É
        context_files.sort(key=lambda x: (x.priority, -x.estimated_lines))
        
        return context_files
    
    def analyze_context_quality(self, selected_files: List[ContextFile]) -> Dict[str, any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å–æ–±—Ä–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""
        metrics = {
            'total_files': len(selected_files),
            'total_lines': sum(f.estimated_lines for f in selected_files),
            'coverage_score': 0.0,
            'completeness_score': 0.0,
            'balance_score': 0.0,
            'expert_analysis_score': 0.0,
            'recommendations': []
        }
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤ (–≤–∫–ª—é—á–∞—è —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑)
        file_types = set(f.file_type for f in selected_files)
        expected_types = {'expert', 'architecture', 'plan', 'opportunities', 'smells'}
        coverage = len(file_types.intersection(expected_types)) / len(expected_types)
        metrics['coverage_score'] = coverage
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ–ª–Ω–æ—Ç—ã (—Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞)
        has_expert = any(f.file_type == 'expert' for f in selected_files)
        has_architecture = any(f.file_type == 'architecture' for f in selected_files)
        has_plan = any(f.file_type == 'plan' for f in selected_files)
        has_problems = any(f.file_type in ['smells', 'opportunities'] for f in selected_files)
        
        # –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–º–µ–µ—Ç –¥–≤–æ–π–Ω–æ–π –≤–µ—Å
        completeness_factors = [has_expert, has_expert, has_architecture, has_plan, has_problems]
        completeness = sum(completeness_factors) / len(completeness_factors)
        metrics['completeness_score'] = completeness
        
        # –û—Ü–µ–Ω–∫–∞ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        if has_expert:
            expert_files = [f for f in selected_files if f.file_type == 'expert']
            has_report = any('report' in f.path for f in expert_files)
            has_json = any(f.path.endswith('.json') for f in expert_files)
            
            expert_score = 0.0
            if has_report:
                expert_score += 0.6  # –û—Ç—á–µ—Ç –≤–∞–∂–Ω–µ–µ
            if has_json:
                expert_score += 0.4  # JSON —Å –¥–∞–Ω–Ω—ã–º–∏
            
            metrics['expert_analysis_score'] = expert_score
        
        # –ê–Ω–∞–ª–∏–∑ –±–∞–ª–∞–Ω—Å–∞ (—Å —É—á–µ—Ç–æ–º —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞)
        type_distribution = {}
        for f in selected_files:
            type_distribution[f.file_type] = type_distribution.get(f.file_type, 0) + 1
        
        # –ò–¥–µ–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
        ideal_ratios = {
            'expert': 0.3,        # –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ - –≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            'architecture': 0.3,  # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
            'plan': 0.2,         # –ü–ª–∞–Ω—ã
            'opportunities': 0.1, # –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
            'smells': 0.1        # –ü—Ä–æ–±–ª–µ–º—ã
        }
        actual_ratios = {k: v/len(selected_files) for k, v in type_distribution.items()}
        
        balance_score = 1.0
        for file_type, ideal_ratio in ideal_ratios.items():
            actual_ratio = actual_ratios.get(file_type, 0)
            balance_score -= abs(ideal_ratio - actual_ratio) * 0.3
        
        metrics['balance_score'] = max(0.0, balance_score)
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        if not has_expert:
            metrics['recommendations'].append("üö® –ö–†–ò–¢–ò–ß–ù–û: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ - –∑–∞–ø—É—Å—Ç–∏—Ç–µ structured_ultimate_analyzer —Å --expert")
        
        if coverage < 0.8:
            metrics['recommendations'].append("–î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è")
        
        if not has_architecture:
            metrics['recommendations'].append("–ö—Ä–∏—Ç–∏—á–Ω–æ: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –¥–∏–∞–≥—Ä–∞–º–º—ã")
        
        if not has_plan:
            metrics['recommendations'].append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: –¥–æ–±–∞–≤—å—Ç–µ –ø–ª–∞–Ω —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞")
        
        if has_expert and metrics['expert_analysis_score'] < 1.0:
            if not any('report' in f.path for f in [f for f in selected_files if f.file_type == 'expert']):
                metrics['recommendations'].append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ—Ç—á–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (.md —Ñ–∞–π–ª)")
            if not any(f.path.endswith('.json') for f in [f for f in selected_files if f.file_type == 'expert']):
                metrics['recommendations'].append("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (.json —Ñ–∞–π–ª)")
        
        if metrics['total_lines'] < 1000:
            metrics['recommendations'].append("–ö–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º (< 1000 —Å—Ç—Ä–æ–∫)")
        
        if metrics['total_lines'] > 3000:
            metrics['recommendations'].append("–ö–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω—ã–º (> 3000 —Å—Ç—Ä–æ–∫)")
        
        self.quality_metrics = metrics
        return metrics
    
    def interactive_file_selection(self) -> List[ContextFile]:
        """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä —Ñ–∞–π–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
        all_files = self.collect_context_files()
        
        print(f"\nüìÅ –ù–∞–π–¥–µ–Ω–æ {len(all_files)} —Ñ–∞–π–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è {self.module_name}")
        print("=" * 60)
        
        selected_files = []
        total_lines = 0
        
        for i, file_info in enumerate(all_files, 1):
            file_path = Path(file_info.path)
            print(f"\n{i:2d}. {file_info.file_type.upper()}: {file_path.name}")
            print(f"    üìÑ ~{file_info.estimated_lines} —Å—Ç—Ä–æ–∫ | –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {file_info.priority}")
            print(f"    üìù {file_info.description}")
            
            if total_lines + file_info.estimated_lines > 2500:
                print(f"    ‚ö†Ô∏è  –ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ –ª–∏–º–∏—Ç–∞ —Å—Ç—Ä–æ–∫ ({total_lines + file_info.estimated_lines} > 2500)")
            
            choice = input("    –í–∫–ª—é—á–∏—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç? [y/N/q]: ").strip().lower()
            
            if choice == 'q':
                break
            elif choice in ['y', 'yes', '–¥', '–¥–∞']:
                selected_files.append(file_info)
                total_lines += file_info.estimated_lines
                print(f"    ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ (–≤—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_lines})")
            else:
                print("    ‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–æ")
        
        print(f"\nüìä –ò—Ç–æ–≥–æ –≤—ã–±—Ä–∞–Ω–æ: {len(selected_files)} —Ñ–∞–π–ª–æ–≤, ~{total_lines} —Å—Ç—Ä–æ–∫")
        return selected_files
    
    def preview_context(self, selected_files: List[ContextFile], lines_per_file: int = 10) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
        preview = f"# Preview: Context for {self.module_name} Refactoring\n\n"
        
        for i, file_info in enumerate(selected_files, 1):
            file_path = Path(file_info.path)
            preview += f"## {i}. {file_path.name} ({file_info.file_type})\n"
            preview += f"**Lines**: ~{file_info.estimated_lines} | **Priority**: {file_info.priority}\n\n"
            
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    lines = []
                    for line_num, line in enumerate(f, 1):
                        lines.append(line.rstrip())
                        if line_num >= lines_per_file:
                            break
                    
                    preview += "```\n"
                    preview += "\n".join(lines)
                    if file_info.estimated_lines > lines_per_file:
                        preview += f"\n... ({file_info.estimated_lines - lines_per_file} more lines)"
                    preview += "\n```\n\n"
                    
            except Exception as e:
                preview += f"*Error reading file: {e}*\n\n"
        
        return preview
    
    def export_context_json(self, selected_files: List[ContextFile], output_path: str) -> None:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ JSON —Ñ–æ—Ä–º–∞—Ç —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–∞–∑–º–µ—Ä–∞."""
        context_data = {
            'module_info': {
                'name': self.module_name,
                'path': str(self.target_module),
                'analysis_dir': str(self.analysis_dir)
            },
            'files': [],
            'quality_metrics': self.quality_metrics,
            'total_lines': sum(f.estimated_lines for f in selected_files)
        }
        
        for file_info in selected_files:
            file_data = asdict(file_info)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–∞–∑–º–µ—Ä–∞
            try:
                with open(file_info.path, 'r', encoding='utf-8', errors='ignore') as f:
                    if file_info.file_type in ['smells', 'duplicates']:
                        # –î–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
                        max_lines = file_info.estimated_lines
                        content_lines = []
                        for line_num, line in enumerate(f, 1):
                            content_lines.append(line.rstrip())
                            if line_num >= max_lines:
                                break
                        file_data['content'] = '\n'.join(content_lines)
                        file_data['truncated'] = True
                        file_data['truncated_at_lines'] = max_lines
                    else:
                        # –î–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –±–µ—Ä–µ–º –ø–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ, –Ω–æ —Å —Ä–∞–∑—É–º–Ω—ã–º –ª–∏–º–∏—Ç–æ–º
                        content = f.read()
                        if len(content) > 100000:  # –ë–æ–ª—å—à–µ 100KB
                            file_data['content'] = content[:100000] + "\n\n... [TRUNCATED: Content too large]"
                            file_data['truncated'] = True
                        else:
                            file_data['content'] = content
                            file_data['truncated'] = False
            except Exception as e:
                file_data['content'] = f"Error reading file: {e}"
                file_data['truncated'] = False
            
            context_data['files'].append(file_data)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(context_data, f, indent=2, ensure_ascii=False)
    
    def generate_quality_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""
        if not self.quality_metrics:
            return "Quality metrics not available. Run analyze_context_quality() first."
        
        metrics = self.quality_metrics
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É —Å —É—á–µ—Ç–æ–º —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        base_score = (
            metrics['coverage_score'] * 0.3 +
            metrics['completeness_score'] * 0.4 +
            metrics['balance_score'] * 0.2
        )
        
        # –ë–æ–Ω—É—Å –∑–∞ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        expert_bonus = metrics.get('expert_analysis_score', 0) * 0.1
        overall_score = base_score + expert_bonus
        
        if overall_score >= 0.9:
            grade = "üü¢ –ü—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–π (—Å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º)"
        elif overall_score >= 0.8:
            grade = "üü¢ –û—Ç–ª–∏—á–Ω—ã–π"
        elif overall_score >= 0.6:
            grade = "üü° –•–æ—Ä–æ—à–∏–π"
        else:
            grade = "üî¥ –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è"
        
        report = f"""# –û—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

## –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {grade} ({overall_score:.1%})

### –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
- **–ü–æ–∫—Ä—ã—Ç–∏–µ —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤**: {metrics['coverage_score']:.1%}
- **–ü–æ–ª–Ω–æ—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞**: {metrics['completeness_score']:.1%}
- **–ë–∞–ª–∞–Ω—Å —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ**: {metrics['balance_score']:.1%}"""
        
        if 'expert_analysis_score' in metrics:
            report += f"\n- **–≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑**: {metrics['expert_analysis_score']:.1%}"
        
        report += f"""

### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- **–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤**: {metrics['total_files']}
- **–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫**: {metrics['total_lines']:,}

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
"""
        
        if metrics['recommendations']:
            for rec in metrics['recommendations']:
                report += f"- {rec}\n"
        else:
            report += "- –ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–ø—Ç–∏–º–∞–ª–µ–Ω –¥–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ ‚úÖ\n"
        
        return report
    
    def create_context_bundle(self, max_lines: int = 2500) -> Tuple[List[ContextFile], int]:
        """–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä —Ñ–∞–π–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞ —Å—Ç—Ä–æ–∫ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""
        all_files = self.collect_context_files()
        selected_files = []
        total_lines = 0
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 0: –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–Ω–∞–∏–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        for file_info in all_files:
            if file_info.priority == 0:
                if total_lines + file_info.estimated_lines <= max_lines:
                    selected_files.append(file_info)
                    total_lines += file_info.estimated_lines
                    print(f"‚úÖ EXPERT: Added {Path(file_info.path).name} (~{file_info.estimated_lines} lines)")
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 1: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ –ø–ª–∞–Ω—ã (–≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        for file_info in all_files:
            if file_info.priority == 1:
                if total_lines + file_info.estimated_lines <= max_lines:
                    selected_files.append(file_info)
                    total_lines += file_info.estimated_lines
                    print(f"‚úÖ HIGH: Added {Path(file_info.path).name} (~{file_info.estimated_lines} lines)")
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 2+: –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã (—Å—Ä–µ–¥–Ω–∏–π –∏ –Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        for file_info in all_files:
            if file_info.priority >= 2:
                if total_lines + file_info.estimated_lines <= max_lines:
                    selected_files.append(file_info)
                    total_lines += file_info.estimated_lines
                    print(f"‚úÖ MED: Added {Path(file_info.path).name} (~{file_info.estimated_lines} lines)")
        
        return selected_files, total_lines
    
    def generate_context_summary(self, selected_files: List[ContextFile], total_lines: int) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤–æ–¥–∫—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è LLM —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""
        summary = f"""# Advanced Context Bundle for {self.module_name} Refactoring

## Target Module
- **File**: `{self.target_module}`
- **Module**: {self.module_name}

## Context Files ({len(selected_files)} files, ~{total_lines} lines)

"""
        
        # –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0)
        expert_files = [f for f in selected_files if f.priority == 0]
        if expert_files:
            summary += "### üéØ Expert Refactoring Analysis (CRITICAL)\n"
            for file_info in expert_files:
                summary += f"- **{file_info.file_type.title()}**: `{Path(file_info.path).name}` (~{file_info.estimated_lines} lines)\n"
                summary += f"  - {file_info.description}\n"
                if 'expert_analysis_report' in file_info.path:
                    summary += "  - üö® **CONTAINS**: Call graph, circular dependencies, external usage, test coverage, characterization tests\n"
                elif 'expert_analysis' in file_info.path and file_info.path.endswith('.json'):
                    summary += "  - üìä **CONTAINS**: Quality score, risk assessment, detailed analysis data\n"
            summary += "\n"
        
        # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ –ø–ª–∞–Ω—ã)
        high_priority = [f for f in selected_files if f.priority == 1]
        if high_priority:
            summary += "### üèóÔ∏è High Priority Files (Architecture & Plans)\n"
            for file_info in high_priority:
                summary += f"- **{file_info.file_type.title()}**: `{Path(file_info.path).name}` (~{file_info.estimated_lines} lines)\n"
                summary += f"  - {file_info.description}\n"
            summary += "\n"
        
        # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (–∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º)
        medium_priority = [f for f in selected_files if f.priority == 2]
        if medium_priority:
            summary += "### üîç Medium Priority Files (Problem Analysis)\n"
            for file_info in medium_priority:
                summary += f"- **{file_info.file_type.title()}**: `{Path(file_info.path).name}` (~{file_info.estimated_lines} lines)\n"
                summary += f"  - {file_info.description}\n"
            summary += "\n"
        
        # –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏ –¥–µ—Ç–∞–ª–∏)
        low_priority = [f for f in selected_files if f.priority >= 3]
        if low_priority:
            summary += "### üìö Supporting Files (Documentation & Details)\n"
            for file_info in low_priority:
                summary += f"- **{file_info.file_type.title()}**: `{Path(file_info.path).name}` (~{file_info.estimated_lines} lines)\n"
                summary += f"  - {file_info.description}\n"
            summary += "\n"
        
        summary += """## üéØ Expert Analysis Highlights

"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –Ω–∞—Ö–æ–¥–∫–∏ –∏–∑ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
        expert_json_files = [f for f in expert_files if f.path.endswith('.json')]
        if expert_json_files:
            try:
                expert_json_path = expert_json_files[0].path
                with open(expert_json_path, 'r', encoding='utf-8') as f:
                    expert_data = json.load(f)
                    
                summary += f"- **Quality Score**: {expert_data.get('analysis_quality_score', 'N/A')}/100\n"
                summary += f"- **Risk Level**: {expert_data.get('risk_assessment', 'N/A').upper()}\n"
                
                recommendations = expert_data.get('recommendations', [])
                if recommendations:
                    summary += "- **Key Recommendations**:\n"
                    for rec in recommendations[:3]:  # –ü–µ—Ä–≤—ã–µ 3 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                        summary += f"  - {rec}\n"
                summary += "\n"
            except Exception as e:
                summary += f"- Expert data available but could not parse: {e}\n\n"
        
        summary += f"""## Usage Instructions

1. **First message to LLM**: Send the target module file (`{self.target_module}`)
2. **Second message to LLM**: Send this context bundle with the files listed above

## üö® Critical Refactoring Focus Areas (Based on Expert Analysis)

- **Circular Dependencies**: Review call graph for cycles that must be resolved first
- **External Usage Impact**: Plan changes carefully due to external dependencies  
- **Test Coverage**: Create characterization tests before refactoring
- **Code Duplication**: Significant savings potential identified
- **Architectural Smells**: Systematic issues requiring attention

## üìã Refactoring Approach

1. **Phase 1**: Address circular dependencies (CRITICAL)
2. **Phase 2**: Create/run characterization tests for safety
3. **Phase 3**: Apply expert recommendations systematically
4. **Phase 4**: Validate with external callers
5. **Phase 5**: Optimize duplicates and smells

---
*Generated by Advanced Context Collector with Expert Analysis for {self.module_name} module*
"""
        
        return summary
    
    def create_advanced_bundle(self, output_dir: str = "advanced_context_bundle", 
                             interactive: bool = False, include_preview: bool = True, 
                             export_json: bool = False) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –Ω–∞–±–æ—Ä."""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # –í—ã–±–æ—Ä —Ñ–∞–π–ª–æ–≤
        if interactive:
            selected_files = self.interactive_file_selection()
        else:
            selected_files, _ = self.create_context_bundle()
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
        self.analyze_context_quality(selected_files)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã
        total_lines = sum(f.estimated_lines for f in selected_files)
        summary = self.generate_context_summary(selected_files, total_lines)
        
        with open(output_path / "CONTEXT_SUMMARY.md", 'w', encoding='utf-8') as f:
            f.write(summary)
        
        # –û—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ
        quality_report = self.generate_quality_report()
        with open(output_path / "QUALITY_REPORT.md", 'w', encoding='utf-8') as f:
            f.write(quality_report)
        
        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä
        if include_preview:
            preview = self.preview_context(selected_files)
            with open(output_path / "CONTEXT_PREVIEW.md", 'w', encoding='utf-8') as f:
                f.write(preview)
        
        # JSON —ç–∫—Å–ø–æ—Ä—Ç
        if export_json:
            self.export_context_json(selected_files, output_path / "context_data.json")
        
        # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –∂–µ—Å—Ç–∫–∏–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–∞–∑–º–µ—Ä–∞
        for i, file_info in enumerate(selected_files, 1):
            source_path = Path(file_info.path)
            if source_path.exists():
                dest_name = f"{i:02d}_{source_path.name}"
                dest_path = output_path / dest_name
                
                try:
                    # –î–ª—è –±–æ–ª—å—à–∏—Ö JSON —Ñ–∞–π–ª–æ–≤ –≤—Å–µ–≥–¥–∞ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
                    if file_info.file_type in ['smells', 'duplicates']:
                        max_lines = file_info.estimated_lines
                        with open(source_path, 'r', encoding='utf-8', errors='ignore') as src:
                            lines = []
                            for line_num, line in enumerate(src, 1):
                                lines.append(line)
                                if line_num >= max_lines:
                                    break
                        
                        with open(dest_path, 'w', encoding='utf-8') as dst:
                            dst.writelines(lines)
                            if line_num >= max_lines:
                                dst.write(f"\n\n... [TRUNCATED: File too large, showing only first {max_lines} lines] ...")
                    else:
                        # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –∏ –ø–ª–∞–Ω–æ–≤
                        import shutil
                        shutil.copy2(source_path, dest_path)
                        
                except Exception as e:
                    print(f"Warning: Could not copy {source_path}: {e}")
        
        print(f"\nüéâ Advanced context bundle created: {output_path}")
        print(f"üìä Quality score: {self.quality_metrics.get('coverage_score', 0):.1%}")
        print(f"üìÅ Files: {len(selected_files)} | Lines: {total_lines:,}")
        
        return str(output_path / "CONTEXT_SUMMARY.md")


def main():
    parser = argparse.ArgumentParser(description="Advanced context collector for module refactoring")
    parser.add_argument("target_module", help="Path to the target module file")
    parser.add_argument("--analysis-dir", help="Analysis results directory")
    parser.add_argument("--output-dir", default="advanced_context_bundle", help="Output directory")
    parser.add_argument("--interactive", "-i", action="store_true", help="Interactive file selection")
    parser.add_argument("--no-preview", action="store_true", help="Skip context preview generation")
    parser.add_argument("--export-json", action="store_true", help="Export context as JSON")
    parser.add_argument("--max-lines", type=int, default=2500, help="Maximum lines in context")
    
    args = parser.parse_args()
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏–º –ø–∞–ø–∫—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
    analysis_dir = args.analysis_dir
    if not analysis_dir:
        temp_collector = AdvancedContextCollector(".", args.target_module)
        analysis_dir = temp_collector.find_analysis_results_dir()
        if not analysis_dir:
            print("‚ùå Error: Could not find analysis results directory")
            return 1
        print(f"üîç Auto-detected analysis directory: {analysis_dir}")
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–±–æ—Ä—â–∏–∫
    collector = AdvancedContextCollector(analysis_dir, args.target_module)
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –Ω–∞–±–æ—Ä
    summary_path = collector.create_advanced_bundle(
        output_dir=args.output_dir,
        interactive=args.interactive,
        include_preview=not args.no_preview,
        export_json=args.export_json
    )
    
    print(f"\nüìã Summary: {summary_path}")
    print("üöÄ Ready for LLM refactoring!")
    
    return 0


if __name__ == "__main__":
    exit(main())