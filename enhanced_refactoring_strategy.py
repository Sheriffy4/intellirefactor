#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –¥–ª—è IntelliRefactor.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–æ–∑–¥–∞–µ—Ç –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞,
–æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ –∫–æ–¥–∞ –∏ –≤—ã–¥–µ–ª–µ–Ω–∏–∏ —á–µ—Ç–∫–∏—Ö –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–µ–π.
"""

from pathlib import Path
from typing import Dict, List, Any
import ast
import re
import logging
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class MethodAnalysis:
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–∞."""
    name: str
    line_start: int
    line_end: int
    size_lines: int
    complexity_score: int
    is_private: bool
    is_dunder: bool
    calls_methods: List[str]
    uses_attributes: List[str]
    semantic_keywords: List[str]
    responsibility_score: Dict[str, float]

@dataclass
class ComponentPlan:
    """–ü–ª–∞–Ω –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è."""
    name: str
    description: str
    methods: List[str]
    interface_methods: List[str]
    estimated_lines: int
    cohesion_score: float
    dependencies: List[str]

class EnhancedRefactoringStrategy:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º."""
    
    def __init__(self):
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –º–µ—Ç–æ–¥–æ–≤
        self.semantic_patterns = {
            'dispatch_orchestration': {
                'keywords': ['dispatch', 'orchestrat', 'coordinate', 'route', 'execute'],
                'patterns': [r'dispatch.*', r'.*_internal', r'.*_wrapper'],
                'description': '–û—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è –∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∞—Ç–∞–∫',
                'weight': 1.0
            },
            'strategy_resolution': {
                'keywords': ['strategy', 'resolve', 'parse', 'combo', 'recipe'],
                'patterns': [r'.*strategy.*', r'resolve.*', r'parse.*', r'.*combo.*'],
                'description': '–†–∞–∑—Ä–µ—à–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∏ —Ä–µ—Ü–µ–ø—Ç–æ–≤',
                'weight': 0.9
            },
            'parameter_management': {
                'keywords': ['param', 'normalize', 'validate', 'map', 'filter'],
                'patterns': [r'.*param.*', r'normalize.*', r'validate.*', r'map.*'],
                'description': '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∞—Ç–∞–∫',
                'weight': 0.8
            },
            'tls_protocol_handling': {
                'keywords': ['tls', 'sni', 'cipher', 'extension', 'hostname', 'clienthello'],
                'patterns': [r'.*sni.*', r'.*tls.*', r'.*cipher.*', r'.*hostname.*'],
                'description': '–û–±—Ä–∞–±–æ—Ç–∫–∞ TLS –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –∏ SNI',
                'weight': 0.9
            },
            'attack_execution': {
                'keywords': ['attack', 'execute', 'primitive', 'advanced', 'technique'],
                'patterns': [r'.*attack.*', r'execute.*', r'.*primitive.*'],
                'description': '–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∞—Ç–∞–∫ –∏ —Ç–µ—Ö–Ω–∏–∫',
                'weight': 0.8
            },
            'logging_monitoring': {
                'keywords': ['log', 'monitor', 'trace', 'correlation', 'metadata'],
                'patterns': [r'.*log.*', r'.*correlation.*', r'.*metadata.*'],
                'description': '–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥',
                'weight': 0.7
            },
            'data_processing': {
                'keywords': ['parse', 'extract', 'find', 'position', 'offset'],
                'patterns': [r'.*parse.*', r'find.*', r'extract.*', r'.*position.*'],
                'description': '–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö',
                'weight': 0.6
            },
            'utility_helpers': {
                'keywords': ['helper', 'util', 'support', 'create', 'generate'],
                'patterns': [r'.*helper.*', r'create.*', r'generate.*', r'.*util.*'],
                'description': '–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏',
                'weight': 0.5
            }
        }
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
        self.min_methods_per_component = 2  # –°–Ω–∏–∂–∞–µ–º –¥–æ 2
        self.min_lines_per_component = 30   # –°–Ω–∏–∂–∞–µ–º –¥–æ 30
        self.min_cohesion_score = 0.1       # –°–Ω–∏–∂–∞–µ–º –¥–æ 0.1
        
    def analyze_method(self, method_node: ast.FunctionDef, content: str) -> MethodAnalysis:
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–∞."""
        
        lines = content.splitlines()
        method_lines = lines[method_node.lineno-1:getattr(method_node, 'end_lineno', method_node.lineno+10)]
        method_content = '\n'.join(method_lines)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—ã–∑–æ–≤—ã –º–µ—Ç–æ–¥–æ–≤
        calls = re.findall(r'self\.([a-zA-Z_][a-zA-Z0-9_]*)\s*\(', method_content)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–æ–≤
        attributes = re.findall(r'self\.([a-zA-Z_][a-zA-Z0-9_]*)', method_content)
        attributes = [attr for attr in attributes if attr not in calls]  # –ò—Å–∫–ª—é—á–∞–µ–º –≤—ã–∑–æ–≤—ã –º–µ—Ç–æ–¥–æ–≤
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        semantic_keywords = self._extract_semantic_keywords(method_content, method_node.name)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ü–µ–Ω–∫–∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏
        responsibility_score = self._calculate_responsibility_scores(method_node.name, method_content)
        
        return MethodAnalysis(
            name=method_node.name,
            line_start=method_node.lineno,
            line_end=getattr(method_node, 'end_lineno', method_node.lineno + 10),
            size_lines=len(method_lines),
            complexity_score=self._calculate_complexity(method_content),
            is_private=method_node.name.startswith('_'),
            is_dunder=method_node.name.startswith('__') and method_node.name.endswith('__'),
            calls_methods=list(set(calls)),
            uses_attributes=list(set(attributes)),
            semantic_keywords=semantic_keywords,
            responsibility_score=responsibility_score
        )
    
    def _extract_semantic_keywords(self, content: str, method_name: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –º–µ—Ç–æ–¥–∞."""
        keywords = []
        content_lower = content.lower()
        name_lower = method_name.lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∏–º–µ–Ω–∏ –º–µ—Ç–æ–¥–∞
        for word in re.findall(r'[a-zA-Z]+', name_lower):
            if len(word) > 2:
                keywords.append(word)
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏ —Å—Ç—Ä–æ–∫
        comments = re.findall(r'#\s*(.+)', content)
        strings = re.findall(r'["\']([^"\']+)["\']', content)
        
        for text in comments + strings:
            for word in re.findall(r'[a-zA-Z]+', text.lower()):
                if len(word) > 3:
                    keywords.append(word)
        
        return list(set(keywords))
    
    def _calculate_responsibility_scores(self, method_name: str, content: str) -> Dict[str, float]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫–∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""
        scores = {}
        content_lower = content.lower()
        name_lower = method_name.lower()
        
        for category, config in self.semantic_patterns.items():
            score = 0.0
            
            # –û—Ü–µ–Ω–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            for keyword in config['keywords']:
                if keyword in name_lower:
                    score += 2.0  # –í—ã—Å–æ–∫–∏–π –≤–µ—Å –¥–ª—è –∏–º–µ–Ω–∏ –º–µ—Ç–æ–¥–∞
                if keyword in content_lower:
                    score += 1.0
            
            # –û—Ü–µ–Ω–∫–∞ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
            for pattern in config['patterns']:
                if re.search(pattern, name_lower):
                    score += 1.5
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º –≤–µ—Å–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            scores[category] = score * config['weight']
        
        return scores
    
    def _calculate_complexity(self, content: str) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞."""
        complexity = 1
        
        # –£—Å–ª–æ–≤–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        complexity += content.count('if ')
        complexity += content.count('elif ')
        complexity += content.count('else:')
        
        # –¶–∏–∫–ª—ã
        complexity += content.count('for ')
        complexity += content.count('while ')
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        complexity += content.count('try:')
        complexity += content.count('except')
        complexity += content.count('finally:')
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –º–µ–Ω–µ–¥–∂–µ—Ä—ã
        complexity += content.count('with ')
        
        # –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã
        complexity += content.count(' and ')
        complexity += content.count(' or ')
        
        return complexity
    
    def group_methods_by_responsibility(self, methods: List[MethodAnalysis]) -> Dict[str, List[str]]:
        """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –º–µ—Ç–æ–¥—ã –ø–æ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—è–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""
        
        groups = {category: [] for category in self.semantic_patterns.keys()}
        unassigned = []
        
        for method in methods:
            # –ù–∞—Ö–æ–¥–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π
            if method.responsibility_score:
                best_category = max(method.responsibility_score.items(), key=lambda x: x[1])
                
                if best_category[1] > 0.5:  # –°–Ω–∏–∂–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                    groups[best_category[0]].append(method.name)
                else:
                    unassigned.append(method.name)
            else:
                unassigned.append(method.name)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–µ–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã
        if unassigned:
            groups['utility_helpers'].extend(unassigned)
        
        # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ –≥—Ä—É–ø–ø—ã
        return {k: v for k, v in groups.items() if v}
    
    def calculate_cohesion_score(self, method_names: List[str], methods: List[MethodAnalysis]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –≥—Ä—É–ø–ø—ã –º–µ—Ç–æ–¥–æ–≤."""
        
        if len(method_names) < 2:
            return 0.0
        
        group_methods = [m for m in methods if m.name in method_names]
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∑–∞–∏–º–Ω—ã–µ –≤—ã–∑–æ–≤—ã
        call_connections = 0
        total_possible_connections = len(group_methods) * (len(group_methods) - 1)
        
        for method in group_methods:
            for other_method in group_methods:
                if other_method.name in method.calls_methods:
                    call_connections += 1
        
        call_cohesion = call_connections / max(total_possible_connections, 1)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±—â–∏–µ –∞—Ç—Ä–∏–±—É—Ç—ã
        all_attributes = set()
        for method in group_methods:
            all_attributes.update(method.uses_attributes)
        
        if all_attributes:
            shared_attributes = 0
            for attr in all_attributes:
                using_methods = sum(1 for m in group_methods if attr in m.uses_attributes)
                if using_methods > 1:
                    shared_attributes += using_methods - 1
            
            attr_cohesion = shared_attributes / (len(all_attributes) * len(group_methods))
        else:
            attr_cohesion = 0.0
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å
        semantic_cohesion = self._calculate_semantic_cohesion(group_methods)
        
        # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏
        return (call_cohesion * 0.4 + attr_cohesion * 0.3 + semantic_cohesion * 0.3)
    
    def _calculate_semantic_cohesion(self, methods: List[MethodAnalysis]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å–≤—è–∑–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–æ–≤."""
        
        if len(methods) < 2:
            return 0.0
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        all_keywords = set()
        for method in methods:
            all_keywords.update(method.semantic_keywords)
        
        if not all_keywords:
            return 0.0
        
        # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        shared_keywords = 0
        for keyword in all_keywords:
            methods_with_keyword = sum(1 for m in methods if keyword in m.semantic_keywords)
            if methods_with_keyword > 1:
                shared_keywords += methods_with_keyword - 1
        
        return shared_keywords / (len(all_keywords) * len(methods))
    
    def create_component_plans(self, groups: Dict[str, List[str]], methods: List[MethodAnalysis]) -> List[ComponentPlan]:
        """–°–æ–∑–¥–∞–µ—Ç –ø–ª–∞–Ω—ã –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä—É–ø–ø –º–µ—Ç–æ–¥–æ–≤."""
        
        component_plans = []
        
        for group_name, method_names in groups.items():
            if len(method_names) < self.min_methods_per_component:
                continue
            
            group_methods = [m for m in methods if m.name in method_names]
            total_lines = sum(m.size_lines for m in group_methods)
            
            if total_lines < self.min_lines_per_component:
                continue
            
            cohesion_score = self.calculate_cohesion_score(method_names, methods)
            
            if cohesion_score < self.min_cohesion_score:
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É–±–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã (–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å)
            interface_methods = [m.name for m in group_methods if not m.is_private and not m.is_dunder]
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
            dependencies = set()
            for method in group_methods:
                for call in method.calls_methods:
                    if call not in method_names:  # –í–Ω–µ—à–Ω–∏–π –≤—ã–∑–æ–≤
                        dependencies.add(call)
            
            component_plan = ComponentPlan(
                name=self._generate_component_name(group_name),
                description=self.semantic_patterns[group_name]['description'],
                methods=method_names,
                interface_methods=interface_methods,
                estimated_lines=total_lines,
                cohesion_score=cohesion_score,
                dependencies=list(dependencies)
            )
            
            component_plans.append(component_plan)
        
        return component_plans
    
    def _generate_component_name(self, group_name: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–º—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä—É–ø–ø—ã."""
        name_mapping = {
            'dispatch_orchestration': 'AttackOrchestrator',
            'strategy_resolution': 'StrategyResolver',
            'parameter_management': 'ParameterManager',
            'tls_protocol_handling': 'TlsProtocolHandler',
            'attack_execution': 'AttackExecutor',
            'logging_monitoring': 'LoggingMonitor',
            'data_processing': 'DataProcessor',
            'utility_helpers': 'UtilityHelper'
        }
        return name_mapping.get(group_name, f'{group_name.title()}Service')
    
    def generate_enhanced_refactoring_config(self, file_path: Path) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞."""
        
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        content = file_path.read_text(encoding='utf-8')
        tree = ast.parse(content)
        
        # –ù–∞—Ö–æ–¥–∏–º –∫–ª–∞—Å—Å AttackDispatcher
        target_class = None
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef) and node.name == "AttackDispatcher":
                target_class = node
                break
        
        if not target_class:
            raise ValueError("AttackDispatcher class not found")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –º–µ—Ç–æ–¥—ã
        methods = []
        for item in target_class.body:
            if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                method_analysis = self.analyze_method(item, content)
                methods.append(method_analysis)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –º–µ—Ç–æ–¥—ã
        groups = self.group_methods_by_responsibility(methods)
        
        # –°–æ–∑–¥–∞–µ–º –ø–ª–∞–Ω—ã –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        component_plans = self.create_component_plans(groups, methods)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è IntelliRefactor
        enhanced_config = {
            'god_class_threshold': 5,  # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥
            'min_methods_for_extraction': 3,  # –ú–∏–Ω–∏–º—É–º 3 –º–µ—Ç–æ–¥–∞ –Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
            'extract_private_methods': True,
            'cohesion_cluster_other': True,
            'cohesion_similarity_threshold': 0.25,  # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            
            # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏
            'responsibility_keywords': {
                component.name.lower().replace('service', '').replace('handler', '').replace('manager', ''): 
                self._extract_keywords_from_methods(component.methods, methods)
                for component in component_plans
            },
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞
            'enhanced_refactoring': {
                'total_methods_analyzed': len(methods),
                'proposed_components': len(component_plans),
                'expected_extraction_rate': sum(len(cp.methods) for cp in component_plans) / len(methods),
                'component_plans': [
                    {
                        'name': cp.name,
                        'description': cp.description,
                        'methods': cp.methods,
                        'interface_methods': cp.interface_methods,
                        'estimated_lines': cp.estimated_lines,
                        'cohesion_score': cp.cohesion_score,
                        'dependencies': cp.dependencies
                    }
                    for cp in component_plans
                ]
            }
        }
        
        return enhanced_config
    
    def _extract_keywords_from_methods(self, method_names: List[str], all_methods: List[MethodAnalysis]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≥—Ä—É–ø–ø—ã –º–µ—Ç–æ–¥–æ–≤."""
        keywords = set()
        
        for method_name in method_names:
            method = next((m for m in all_methods if m.name == method_name), None)
            if method:
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ –∏–∑ –∏–º–µ–Ω–∏ –º–µ—Ç–æ–¥–∞
                name_words = re.findall(r'[a-zA-Z]+', method_name.lower())
                keywords.update(word for word in name_words if len(word) > 2)
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                keywords.update(method.semantic_keywords[:5])  # –¢–æ–ø-5 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        
        return list(keywords)[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 10 –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏

def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞."""
    
    strategy = EnhancedRefactoringStrategy()
    file_path = Path('core/bypass/engine/attack_dispatcher.py')
    
    if not file_path.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return
    
    try:
        config = strategy.generate_enhanced_refactoring_config(file_path)
        
        print("üöÄ –£–õ–£–ß–®–ï–ù–ù–ê–Ø –°–¢–†–ê–¢–ï–ì–ò–Ø –†–ï–§–ê–ö–¢–û–†–ò–ù–ì–ê")
        print("=" * 50)
        
        enhanced = config['enhanced_refactoring']
        print("üìä –ê–Ω–∞–ª–∏–∑:")
        print(f"  –í—Å–µ–≥–æ –º–µ—Ç–æ–¥–æ–≤: {enhanced['total_methods_analyzed']}")
        print(f"  –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {enhanced['proposed_components']}")
        print(f"  –û–∂–∏–¥–∞–µ–º—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {enhanced['expected_extraction_rate']:.1%}")
        
        print("\nüèóÔ∏è –ü–ª–∞–Ω—ã –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:")
        for plan in enhanced['component_plans']:
            print(f"\nüì¶ {plan['name']}")
            print(f"   {plan['description']}")
            print(f"   –ú–µ—Ç–æ–¥–æ–≤: {len(plan['methods'])} (—Å—Ç—Ä–æ–∫: {plan['estimated_lines']})")
            print(f"   –°–≤—è–∑–Ω–æ—Å—Ç—å: {plan['cohesion_score']:.2f}")
            print(f"   –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å: {len(plan['interface_methods'])} –ø—É–±–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤")
            if plan['dependencies']:
                print(f"   –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {len(plan['dependencies'])} –≤–Ω–µ—à–Ω–∏—Ö –≤—ã–∑–æ–≤–æ–≤")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config_file = Path('enhanced_refactoring_config.json')
        import json
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {config_file}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()